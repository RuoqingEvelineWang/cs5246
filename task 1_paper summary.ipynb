{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "seed=25 #for random state / reproducibility\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_colwidth', 200) \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: torch in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: requests in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers sentence-transformers torch nltk\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekabu\\AppData\\Local\\Temp\\ipykernel_47184\\149443613.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"ai_ml_papers.csv\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"ai_ml_papers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1. BERT Embeddings with Text Rank Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ekabu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "# Load a pre-trained BERT model for sentence embeddings\n",
    "model_BERT = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def summarize_BERT_TextRank(text):\n",
    "    text = preprocess(text) #preprocess\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Convert each sentence into an embedding\n",
    "    sentence_embeddings = model_BERT.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "    # Compute the mean embedding (representing the overall document)\n",
    "    mean_embedding = torch.mean(sentence_embeddings, dim=0)\n",
    "\n",
    "    # Compute cosine similarity of each sentence to the overall document embedding\n",
    "    cos_similarities = cosine_similarity(sentence_embeddings.cpu().numpy(), mean_embedding.cpu().numpy().reshape(1, -1))\n",
    "\n",
    "    # Rank sentences by similarity\n",
    "    top_n = 5  # Select the top 5 most representative sentences\n",
    "    top_sentence_indices = np.argsort(cos_similarities, axis=0)[-top_n:].flatten()\n",
    "\n",
    "    # Sort indices to maintain the original order of sentences\n",
    "    top_sentence_indices = sorted(top_sentence_indices)\n",
    "\n",
    "    # Extract the summary\n",
    "    summary = \" \".join([sentences[i] for i in top_sentence_indices])\n",
    "\n",
    "    return summary\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2. Using google/pegasus-xsum model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-arxiv and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"google/pegasus-arxiv\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pegasus(text):\n",
    "    text = preprocess(text) #preprocess\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    chunks = chunk_text(sentences, tokenizer)\n",
    "    \n",
    "    summary_list = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], max_length=200, num_beams=5)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summary_list.append(summary)\n",
    "\n",
    "    # Combine all summarized chunks\n",
    "    summary = \" \".join(summary_list)\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 3\n",
    "\n",
    "df_sample = df.sample(n=n_sample, random_state=seed)\n",
    "#extract and append full-text\n",
    "df_sample['full_text'] = df_sample['id'].apply(extract_pdf_text)\n",
    "df_sample['summary_bert_textRank'] = df_sample['full_text'].apply(summarize_BERT_TextRank)\n",
    "df_sample['summary_pegasus_pretrained'] = df_sample['full_text'].apply(summarize_pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id                                                             title  \\\n",
      "91709   2101.10007      Adaptive Scheduling for Machine Learning Tasks over Networks   \n",
      "89614   2012.12109  Enhance Convolutional Neural Networks with Noise Incentive Block   \n",
      "108316  2107.11481          Similarity Based Label Smoothing For Dialogue Generation   \n",
      "\n",
      "                                                                                                                                                                                          summary_bert_textRank  \n",
      "91709   paper , whenever expectation iterates wk taken , expectation data collected time k. b. scheduling problem single task given modeling machine learning task needs solved , scheduling problem follows...  \n",
      "89614   figure 1 demonstrates ï¬‚atness degradation affects several typical applications : ( ) semantic image synthesis synthesizes arxiv:2012.12109v2 [ cs.cv ] 9 jun 2021 photo-realistic textures piece-wis...  \n",
      "108316  language modelling , incorporating label smoothing \u0001\u0002 uniform probability incorrect classes convey incorrect knowledge model . paper present ways \u0001\u0002 information modifying data \u0001\u0002 uniform distribut...  \n",
      "                id                                                             title  \\\n",
      "91709   2101.10007      Adaptive Scheduling for Machine Learning Tasks over Networks   \n",
      "89614   2012.12109  Enhance Convolutional Neural Networks with Noise Incentive Block   \n",
      "108316  2107.11481          Similarity Based Label Smoothing For Dialogue Generation   \n",
      "\n",
      "                                                                                                                                                                                     summary_pegasus_pretrained  \n",
      "91709   communication efficiency addressed informativeness data . building scheduling algorithms aim update machine learning task whose data carry informative information , minimize expected square predic...  \n",
      "89614   recently , convolutional neural networks ( cnns ) demonstrated great success various image processing computer vision applications . , however , cnns fed flat inputs , may fail generate vivid resu...  \n",
      "108316  in this paper , we present ways modifying data uniform distribution label smoothing appropriate data dependent , effect cross entropy loss mechanism , present knowledge model mechanism . paper pre...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df_sample[['id', 'title', 'summary_bert_textRank']])\n",
    "print(df_sample[['id', 'title', 'summary_pegasus_pretrained']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
