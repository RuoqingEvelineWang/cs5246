{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Notebook Summary: Paper Summarization\n",
    "\n",
    "This notebook presents both extractive and abstractive approaches to generate paper summarization related to AI research. It includes:\n",
    "\n",
    "1. **Text preprocessing**: Text cleaning including text normalization, citations/references removal, and abstract exclusion.\n",
    "2. **SBERT-Based Semantic Embedding and TextRank**: Dense sentence embeddings using `all-MiniLM-L6-v2` and Text Rank method to extract key sentences.\n",
    "3. **Pegasus pre-trained model**: Abstractive summarization using Pegasus pre-trained model.\n",
    "4. **Evaluation**: Measures performance of generated summary as compared to the Abstract using ROUGE, BLEU, and BERT scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "seed=7 #for random state / reproducibility #curr best 8\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_colwidth', 200) \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: torch in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: filelock in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.10.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: click in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers sentence-transformers torch nltk\n",
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekabu\\AppData\\Local\\Temp\\ipykernel_1704\\149443613.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"ai_ml_papers.csv\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"ai_ml_papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103953    Bayesian Attention Belief Networks\\nShujian Zhang * 1 Xinjie Fan * 1 Bo Chen 2 Mingyuan Zhou 1\\nAbstract\\nAttention-based neural networks have achieved\\nstate-of-the-art results on a wide range of...\n",
      "143223    1 \\n \\n \\nResearch on Stable Obstacle Avoidance Control \\nStrategy for Tracked Intelligent Transportation \\nVehicles in Non-structural Environment Based on \\nDeep Learning \\nYitian Wang, Jun Lin, ...\n",
      "130405    Introduction\\nReinforcement learning (RL) agents have reached superhuman performance in many tasks, such as\\ngames, with clearly deÔ¨Åned objectives [23, 3, 26]. However, real-world deployment of RL...\n",
      "Name: removed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import extract_sections\n",
    "\n",
    "#Extract paper content by excluding abstract and references\n",
    "def remove_abstract_and_references(text, sections):\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    intro_lines = [line for line, section in sections if section.upper() == 'INTRODUCTION']\n",
    "    intro_line = min(intro_lines) if intro_lines else None #position if intro\n",
    "    \n",
    "    ref_lines = [line for line, section in sections if section.upper() == 'REFERENCES' or section == \"BIBLIOGRAPHY\" or section == \"ACKNOWLEDGEMENTS\"]\n",
    "    ref_line = min(ref_lines) if ref_lines else None #position of references\n",
    "\n",
    "    if intro_line and ref_line: #extract paper only from introduction and exclude reference\n",
    "        trimmed_lines = lines[intro_line : (ref_line - 1)] \n",
    "    elif intro_line:\n",
    "        trimmed_lines = lines[intro_line:]\n",
    "    elif ref_line:\n",
    "        trimmed_lines = lines[:ref_line]\n",
    "    else:\n",
    "        trimmed_lines = lines\n",
    "    \n",
    "    return '\\n'.join(trimmed_lines)\n",
    "\n",
    "n_sample = 3\n",
    "\n",
    "df_sample = df.sample(n=n_sample, random_state=seed)\n",
    "#extract and append full-text\n",
    "df_sample['full_text'] = df_sample['id'].apply(extract_pdf_text)\n",
    "\n",
    "df_sample['sections'] = df_sample['full_text'].apply(extract_sections)\n",
    "#print(df_sample['full_text'].iloc[1])\n",
    "df_sample['removed'] = df_sample.apply(\n",
    "    lambda row: remove_abstract_and_references(row['full_text'], row['sections']), axis=1\n",
    ")\n",
    "print(df_sample['removed'])\n",
    "\n",
    "#extract_sections(df_sample['full_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ekabu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#tokenization and text cleaning\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    # Lowercase and remove URLs/special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    #Removing Extra Spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    #Remove single characters or digits, tend to be part of formula\n",
    "    text = re.sub(r'\\b[a-z0-9]\\b', '', text)\n",
    "    \n",
    "    #Remove -\\n (dash and newline) \n",
    "    # For example: au-\\ntonomous -> autonomous\n",
    "    text = re.sub(r'(\\w+)-\\s*(\\w+)', r'\\1\\2', text)\n",
    "    \n",
    "    #Remove Citations\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)  # Removes [12] , [2-5]\n",
    "    text = re.sub(r'\\([\\w\\s,.]+,\\s\\d{4}\\s?\\)', '', text) # Removes (Author, Year) or (et al., 2023)\n",
    "    text = re.sub(r'\\[[\\w\\s,.]+,\\s\\d{4}\\s?\\]', '', text) # Removes [Author, Year] or [et al., 2023]\n",
    "    text = re.sub(r'et al.,', '', text) # Removes et al. in general\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1. BERT Embeddings with Text Rank Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# Load a pre-trained BERT model for sentence embeddings\n",
    "model_BERT = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def summarize_BERT_TextRank(text):\n",
    "    text = preprocess(text) #preprocess\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Convert each sentence into an embedding\n",
    "    sentence_embeddings = model_BERT.encode(sentences)\n",
    "\n",
    "    # Compute similarity matrix\n",
    "    sim_matrix = cosine_similarity(sentence_embeddings)\n",
    "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    # Rank sentences using PageRank scores\n",
    "    top_n = 2\n",
    "    ranked = sorted(((scores[i], i, s) for i, s in enumerate(sentences)), reverse=True)  # Include index to track original position\n",
    "    top_indices = [i for (_, i, _) in ranked[:top_n]]  # Extract top 5 indices\n",
    "\n",
    "    # Sort indices to restore original order (e.g., [2, 0, 4] ‚Üí [0, 2, 4])\n",
    "    top_indices_sorted = sorted(top_indices)\n",
    "\n",
    "    # Extract the summary\n",
    "    summary = \" \".join([sentences[i] for i in top_indices_sorted])\n",
    "\n",
    "    return summary\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2. Use pretrained google/pegasus-xsum model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"sshleifer/distill-pegasus-xsum-16-4\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join sentences while keeping within the 1024-token limit\n",
    "def chunk_text(sentences, tokenizer, max_tokens=1024, overlap=100):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize once and cache\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        token_len = len(tokens)\n",
    "        \n",
    "        # Flush chunk if adding this sentence exceeds max_tokens\n",
    "        if current_length + token_len > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            # Retain last `overlap` tokens for context (if possible)\n",
    "            if overlap > 0:\n",
    "                overlap_start = max(0, len(current_chunk) - overlap)\n",
    "                current_chunk = current_chunk[overlap_start:]\n",
    "                current_length = sum(len(tokenizer.tokenize(s)) for s in current_chunk)\n",
    "            else:\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_length += token_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def summarize_pegasus(text):\n",
    "    text = preprocess(text) #preprocess\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    chunks = chunk_text(sentences, tokenizer)\n",
    "    \n",
    "    summary_list = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}  # move inputs to same device\n",
    "        \n",
    "        summary_ids = model.generate(inputs[\"input_ids\"], max_length=50, min_length=30, num_beams=5, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summary_list.append(summary)\n",
    "\n",
    "    # Combine all summarized chunks\n",
    "    summary = \" \".join(summary_list)\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract full-text pdf and Generate sections**  \n",
    "Note: each section shows (line number, section name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>sections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103953</th>\n",
       "      <td>Bayesian Attention Belief Networks</td>\n",
       "      <td>[(2, Abstract), (1142, Acknowledgements), (1151, References)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143223</th>\n",
       "      <td>Bayesian Variable Selection in a Million Dimensions</td>\n",
       "      <td>[(1690, References)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130405</th>\n",
       "      <td>Preprocessing Reward Functions for Interpretability</td>\n",
       "      <td>[(7, Abstract), (21, Introduction), (174, Results), (424, Conclusion), (433, References)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      title                                                                                   sections\n",
       "103953                   Bayesian Attention Belief Networks                              [(2, Abstract), (1142, Acknowledgements), (1151, References)]\n",
       "143223  Bayesian Variable Selection in a Million Dimensions                                                                       [(1690, References)]\n",
       "130405  Preprocessing Reward Functions for Interpretability  [(7, Abstract), (21, Introduction), (174, Results), (424, Conclusion), (433, References)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample = 3\n",
    "\n",
    "df_sample = df.sample(n=n_sample, random_state=seed)\n",
    "#extract and append full-text\n",
    "df_sample['full_text'] = df_sample['id'].apply(extract_pdf_text)\n",
    "df_sample['sections'] = df_sample['full_text'].apply(extract_sections)\n",
    "df_sample[['title','sections']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text preprocessing**  \n",
    "Remove Abstract & References and text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103953</th>\n",
       "      <td>Bayesian Attention Belief Networks</td>\n",
       "      <td>bayesian attention belief networks shujian zhang * xinjie fan * bo chen mingyuan zhou abstract attentionbased neural networks have achieved stateof-theart results on wide range of tasks . most suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143223</th>\n",
       "      <td>Bayesian Variable Selection in a Million Dimensions</td>\n",
       "      <td>research on stable obstacle avoidance control strategy for tracked intelligent transportation vehicles in nonstructural environment based on deep learning yitian wang , jun lin , liu zhang , tianh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130405</th>\n",
       "      <td>Preprocessing Reward Functions for Interpretability</td>\n",
       "      <td>introduction reinforcement learning ( rl ) agents have reached superhuman performance in many tasks , such as games , with clearly deÔ¨Åned objectives [ 23 , , 26 ] . however , realworld deployment ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      title  \\\n",
       "103953                   Bayesian Attention Belief Networks   \n",
       "143223  Bayesian Variable Selection in a Million Dimensions   \n",
       "130405  Preprocessing Reward Functions for Interpretability   \n",
       "\n",
       "                                                                                                                                                                                              preprocessed_text  \n",
       "103953  bayesian attention belief networks shujian zhang * xinjie fan * bo chen mingyuan zhou abstract attentionbased neural networks have achieved stateof-theart results on wide range of tasks . most suc...  \n",
       "143223  research on stable obstacle avoidance control strategy for tracked intelligent transportation vehicles in nonstructural environment based on deep learning yitian wang , jun lin , liu zhang , tianh...  \n",
       "130405  introduction reinforcement learning ( rl ) agents have reached superhuman performance in many tasks , such as games , with clearly deÔ¨Åned objectives [ 23 , , 26 ] . however , realworld deployment ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['preprocessed_text'] = df_sample.apply(\n",
    "    lambda row: remove_abstract_and_references(row['full_text'], row['sections']), axis=1\n",
    ")\n",
    "df_sample['preprocessed_text'] = df_sample['preprocessed_text'].apply(preprocess)\n",
    "df_sample[['title','preprocessed_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id                                                title  \\\n",
      "103953  2106.05251                   Bayesian Attention Belief Networks   \n",
      "143223   2208.0118  Bayesian Variable Selection in a Million Dimensions   \n",
      "130405  2203.13553  Preprocessing Reward Functions for Interpretability   \n",
      "\n",
      "                                                                                                                                                                                          summary_bert_textRank  \n",
      "103953                                                                                                         bayesian attention modules . , and le , . . sequence to sequence learning with neural networks .  \n",
      "143223  15 ( ) , in which the unmanned vehicle successfully plans safe obstacle avoidance path and steers to avoid obstacles at the . based on this figure , the unmanned vehicle is descending and performi...  \n",
      "130405  from , we produce simpler but equivalent reward function ‚Ä≤ , which we then visualize . -. . . . figure 11 : reward models trained on synthetic data from the path reward using preference comparison...  \n"
     ]
    }
   ],
   "source": [
    "df_sample['summary_bert_textRank'] = df_sample['full_text'].apply(summarize_BERT_TextRank)\n",
    "print(df_sample[['id', 'title', 'summary_bert_textRank']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id                                                title  \\\n",
      "103953  2106.05251                   Bayesian Attention Belief Networks   \n",
      "143223   2208.0118  Bayesian Variable Selection in a Million Dimensions   \n",
      "130405  2203.13553  Preprocessing Reward Functions for Interpretability   \n",
      "\n",
      "                                                                                                                                                                                     summary_pegasus_pretrained  \n",
      "103953  We have developed a new system of attention-based neural networks that outperforms deterministic and stateof-theart models , which have become the foundation for many computational tasks. We have ...  \n",
      "143223  A study has been carried out at the University of China to improve the ability of autonomous vehicles to avoid obstacles in non-structural environments and in emergency situations. Obstacle avoida...  \n",
      "130405  In our series of letters from the University of Berkeley, we have proposed a new approach to learning reward functions, which can be more difficult to understand than the original. In our series o...  \n"
     ]
    }
   ],
   "source": [
    "df_sample['summary_pegasus_pretrained'] = df_sample['full_text'].apply(summarize_pegasus)\n",
    "print(df_sample[['id', 'title', 'summary_pegasus_pretrained']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: absl-py in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from rouge_score) (3.7)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from evaluate) (0.29.3)\n",
      "Requirement already satisfied: dill in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from evaluate) (2024.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from evaluate) (24.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.11.16)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert_score in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: requests in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from bert_score) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from bert_score) (3.8.4)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from bert_score) (24.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from bert_score) (4.49.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from bert_score) (2.2.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from pandas>=1.0.1->bert_score) (2022.7)\n",
      "Requirement already satisfied: filelock in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch>=1.0.0->bert_score) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch>=1.0.0->bert_score) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from torch>=1.0.0->bert_score) (2024.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from tqdm>=4.31.1->bert_score) (0.4.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers>=3.0.0->bert_score) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers>=3.0.0->bert_score) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers>=3.0.0->bert_score) (0.29.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from matplotlib->bert_score) (1.2.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from matplotlib->bert_score) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from matplotlib->bert_score) (3.1.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from matplotlib->bert_score) (1.4.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from matplotlib->bert_score) (4.51.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests->bert_score) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests->bert_score) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests->bert_score) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from requests->bert_score) (2.0.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ekabu\\anaconda3\\lib\\site-packages (from sympy->torch>=1.0.0->bert_score) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\ekabu\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge_score\n",
    "%pip install evaluate\n",
    "%pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "from evaluate import load\n",
    "import bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.314\n",
      "ROUGE-2: 0.139\n",
      "ROUGE-L: 0.238\n",
      "BLEU: 0.011\n",
      "BERTScore-F1: 0.892\n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics\n",
    "rouge = load(\"rouge\")\n",
    "bleu = load(\"bleu\")\n",
    "# Sample input\n",
    "abstract = \"\"\"Queenstown, a vibrant tourist destination, thrives on its stunning scenery and diverse activities. However, this reliance on tourism presents a complex dynamic, with both opportunities and challenges for the local community. This paper explores the multifaceted impact of tourism on the Queenstown community, examining its economic benefits alongside the social and environmental consequences.\\n\\nTourism undeniably fuels the local economy, generating significant revenue through various sectors like accommodation, hospitality, and retail. Businesses directly and indirectly benefit from the influx of tourists, creating employment opportunities and stimulating economic growth.\\n\\nThe rapid growth of tourism also brings challenges. Overcrowding can lead to a decline in the quality of life for residents, with increased traffic congestion, strain on infrastructure, and a rise in housing costs pushing locals out. Furthermore, the environmental impact of tourism, such as pollution and habitat destruction, poses a long-term threat to the natural beauty that attracts tourists in the first place.\\n\\nTourism is a double-edged sword for Queenstown. While it provides economic opportunities, it also presents social and environmental challenges that require careful management. A sustainable approach to tourism development is crucial to ensure that the benefits are shared equitably and that the community's quality of life and the environment are protected for future generations.\"\"\"\n",
    "\n",
    "generated_summary = \"\"\"This research paper examines the dual impact of tourism on Queenstown, highlighting its economic benefits (job creation, revenue) alongside the social and environmental challenges (overcrowding, infrastructure strain, environmental degradation). It concludes that a sustainable tourism approach is vital for the community's well-being and the long-term preservation of its natural beauty.\"\"\"\n",
    "\n",
    "# Run evaluation\n",
    "def evaluate_automatic_metrics(abstract, generated_summary):\n",
    "    results = {}\n",
    "\n",
    "    # ROUGE\n",
    "    rouge_scores = rouge.compute(predictions=[generated_summary], references=[abstract])\n",
    "    results[\"ROUGE-1\"] = round(rouge_scores[\"rouge1\"], 3)\n",
    "    results[\"ROUGE-2\"] = round(rouge_scores[\"rouge2\"], 3)\n",
    "    results[\"ROUGE-L\"] = round(rouge_scores[\"rougeL\"], 3)\n",
    "\n",
    "    # BLEU\n",
    "    bleu_score = bleu.compute(predictions=[generated_summary], references=[[abstract]])\n",
    "    results[\"BLEU\"] = round(bleu_score[\"bleu\"], 3)\n",
    "\n",
    "    # BERTScore\n",
    "    P, R, F1 = bert_score.score([generated_summary], [abstract], lang=\"en\", verbose=False)\n",
    "    results[\"BERTScore-F1\"] = round(F1[0].item(), 3)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "auto_scores = evaluate_automatic_metrics(abstract, generated_summary)\n",
    "for metric, score in auto_scores.items():\n",
    "    print(f\"{metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "df_eval = df_sample[['title', 'abstract', 'summary_bert_textRank', 'summary_pegasus_pretrained']].copy()\n",
    "\n",
    "multi_scores_textrank = []\n",
    "multi_scores_bart = []\n",
    "\n",
    "COL_ROUGE1 = 'ROUGE-1'\n",
    "COL_ROUGE2 = 'ROUGE-2'\n",
    "COL_ROUGEL = 'ROUGE-L'\n",
    "COL_BLEU = 'BLEU'\n",
    "COL_BERT = 'BERTScore-F1'\n",
    "COL_TEXTRANK = ' TextRank'\n",
    "COL_PEGASUS = ' Pegasus'\n",
    "\n",
    "df_eval[COL_ROUGE1 + COL_TEXTRANK] = None\n",
    "df_eval[COL_ROUGE1 + COL_PEGASUS] = None\n",
    "\n",
    "df_eval[COL_ROUGE2 + COL_TEXTRANK] = None\n",
    "df_eval[COL_ROUGE2 + COL_PEGASUS] = None\n",
    "\n",
    "df_eval[COL_ROUGEL + COL_TEXTRANK] = None\n",
    "df_eval[COL_ROUGEL + COL_PEGASUS] = None\n",
    "\n",
    "df_eval[COL_BLEU + COL_TEXTRANK] = None\n",
    "df_eval[COL_BLEU + COL_PEGASUS] = None\n",
    "\n",
    "df_eval[COL_BERT + COL_TEXTRANK] = None\n",
    "df_eval[COL_BERT + COL_PEGASUS] = None\n",
    "\n",
    "\n",
    "for index, row in df_eval.iterrows():\n",
    "    auto_scores_textrank = evaluate_automatic_metrics(row['abstract'], row['summary_bert_textRank'])\n",
    "    auto_scores_pegasus = evaluate_automatic_metrics(row['abstract'], row['summary_pegasus_pretrained'])\n",
    "    \n",
    "    df_eval.loc[index, COL_ROUGE1 + COL_TEXTRANK] = auto_scores_textrank[COL_ROUGE1]\n",
    "    df_eval.loc[index, COL_ROUGE1 + COL_PEGASUS] = auto_scores_pegasus[COL_ROUGE1]\n",
    "    \n",
    "    df_eval.loc[index, COL_ROUGE2 + COL_TEXTRANK] = auto_scores_textrank[COL_ROUGE2]\n",
    "    df_eval.loc[index, COL_ROUGE2 + COL_PEGASUS] = auto_scores_pegasus[COL_ROUGE2]\n",
    "    \n",
    "    df_eval.loc[index, COL_ROUGEL + COL_TEXTRANK] = auto_scores_textrank[COL_ROUGEL]\n",
    "    df_eval.loc[index, COL_ROUGEL + COL_PEGASUS] = auto_scores_pegasus[COL_ROUGEL]\n",
    "    \n",
    "    df_eval.loc[index, COL_BLEU + COL_TEXTRANK] = auto_scores_textrank[COL_BLEU]\n",
    "    df_eval.loc[index, COL_BLEU + COL_PEGASUS] = auto_scores_pegasus[COL_BLEU]\n",
    "    \n",
    "    df_eval.loc[index, COL_BERT + COL_TEXTRANK] = auto_scores_textrank[COL_BERT]\n",
    "    df_eval.loc[index, COL_BERT + COL_PEGASUS] = auto_scores_pegasus[COL_BERT]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>summary_bert_textRank</th>\n",
       "      <th>summary_pegasus_pretrained</th>\n",
       "      <th>ROUGE-1 TextRank</th>\n",
       "      <th>ROUGE-1 Pegasus</th>\n",
       "      <th>ROUGE-2 TextRank</th>\n",
       "      <th>ROUGE-2 Pegasus</th>\n",
       "      <th>ROUGE-L TextRank</th>\n",
       "      <th>ROUGE-L Pegasus</th>\n",
       "      <th>BLEU TextRank</th>\n",
       "      <th>BLEU Pegasus</th>\n",
       "      <th>BERTScore-F1 TextRank</th>\n",
       "      <th>BERTScore-F1 Pegasus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103953</th>\n",
       "      <td>Bayesian Attention Belief Networks</td>\n",
       "      <td>Attention-based neural networks have achieved state-of-the-art results on a\\nwide range of tasks. Most such models use deterministic attention while\\nstochastic attention is less explored due to...</td>\n",
       "      <td>bayesian attention modules . , and le , . . sequence to sequence learning with neural networks .</td>\n",
       "      <td>We have developed a new system of attention-based neural networks that outperforms deterministic and stateof-theart models , which have become the foundation for many computational tasks. We have ...</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143223</th>\n",
       "      <td>Bayesian Variable Selection in a Million Dimensions</td>\n",
       "      <td>Bayesian variable selection is a powerful tool for data analysis, as it\\noffers a principled method for variable selection that accounts for prior\\ninformation and uncertainty. However, wider ad...</td>\n",
       "      <td>15 ( ) , in which the unmanned vehicle successfully plans safe obstacle avoidance path and steers to avoid obstacles at the . based on this figure , the unmanned vehicle is descending and performi...</td>\n",
       "      <td>A study has been carried out at the University of China to improve the ability of autonomous vehicles to avoid obstacles in non-structural environments and in emergency situations. Obstacle avoida...</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130405</th>\n",
       "      <td>Preprocessing Reward Functions for Interpretability</td>\n",
       "      <td>In many real-world applications, the reward function is too complex to be\\nmanually specified. In such cases, reward functions must instead be learned\\nfrom human feedback. Since the learned rew...</td>\n",
       "      <td>from , we produce simpler but equivalent reward function ‚Ä≤ , which we then visualize . -. . . . figure 11 : reward models trained on synthetic data from the path reward using preference comparison...</td>\n",
       "      <td>In our series of letters from the University of Berkeley, we have proposed a new approach to learning reward functions, which can be more difficult to understand than the original. In our series o...</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      title  \\\n",
       "103953                   Bayesian Attention Belief Networks   \n",
       "143223  Bayesian Variable Selection in a Million Dimensions   \n",
       "130405  Preprocessing Reward Functions for Interpretability   \n",
       "\n",
       "                                                                                                                                                                                                       abstract  \\\n",
       "103953    Attention-based neural networks have achieved state-of-the-art results on a\\nwide range of tasks. Most such models use deterministic attention while\\nstochastic attention is less explored due to...   \n",
       "143223    Bayesian variable selection is a powerful tool for data analysis, as it\\noffers a principled method for variable selection that accounts for prior\\ninformation and uncertainty. However, wider ad...   \n",
       "130405    In many real-world applications, the reward function is too complex to be\\nmanually specified. In such cases, reward functions must instead be learned\\nfrom human feedback. Since the learned rew...   \n",
       "\n",
       "                                                                                                                                                                                          summary_bert_textRank  \\\n",
       "103953                                                                                                         bayesian attention modules . , and le , . . sequence to sequence learning with neural networks .   \n",
       "143223  15 ( ) , in which the unmanned vehicle successfully plans safe obstacle avoidance path and steers to avoid obstacles at the . based on this figure , the unmanned vehicle is descending and performi...   \n",
       "130405  from , we produce simpler but equivalent reward function ‚Ä≤ , which we then visualize . -. . . . figure 11 : reward models trained on synthetic data from the path reward using preference comparison...   \n",
       "\n",
       "                                                                                                                                                                                     summary_pegasus_pretrained  \\\n",
       "103953  We have developed a new system of attention-based neural networks that outperforms deterministic and stateof-theart models , which have become the foundation for many computational tasks. We have ...   \n",
       "143223  A study has been carried out at the University of China to improve the ability of autonomous vehicles to avoid obstacles in non-structural environments and in emergency situations. Obstacle avoida...   \n",
       "130405  In our series of letters from the University of Berkeley, we have proposed a new approach to learning reward functions, which can be more difficult to understand than the original. In our series o...   \n",
       "\n",
       "       ROUGE-1 TextRank ROUGE-1 Pegasus ROUGE-2 TextRank ROUGE-2 Pegasus ROUGE-L TextRank ROUGE-L Pegasus BLEU TextRank BLEU Pegasus BERTScore-F1 TextRank BERTScore-F1 Pegasus  \n",
       "103953            0.073           0.018            0.021           0.009            0.062           0.013           0.0        0.003                 0.793                0.789  \n",
       "143223            0.126           0.012              0.0           0.001            0.069           0.009           0.0          0.0                 0.795                0.741  \n",
       "130405            0.192           0.024            0.057            0.01            0.124           0.017         0.013        0.003                 0.816                0.786  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_eval.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
