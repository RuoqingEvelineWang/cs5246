{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Notebook Summary: Find Semantically Similar Research Papers\n",
    "\n",
    "This notebook demonstrates a multi-method approach for retrieving semantically similar research papers using their abstracts. It includes:\n",
    "\n",
    "1. **TF-IDF Retrieval**: Traditional cosine similarity on bag-of-words representations.\n",
    "2. **SBERT-Based Semantic Retrieval**: Dense sentence embeddings using `all-MiniLM-L6-v2`.\n",
    "3. **Cross-Encoder Reranking**: Fine-grained relevance scoring using the `cross-encoder/ms-marco` model.\n",
    "4. **Explainability with KeyBERT**: Visual inspection of overlapping keywords between query and candidate abstracts.\n",
    "5. **Evaluation**: Measures retrieval effectiveness using average cosine similarity.\n",
    "\n",
    "Each retrieval method is modular, and the notebook can be easily extended for additional models, visualizations, or use cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import *\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions import preprocess, get_number_of_times_a_paper_is_cited\n",
    "from keybert import KeyBERT\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "\n",
    "seed=25 #for random state\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_colwidth', 200) \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8l/vb4tv7t57rscnzp2p_mm47g00000gn/T/ipykernel_15475/2656871807.py:5: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Load and preprocess dataset\n",
    "# -----------------------------\n",
    "file_path = \"ai_ml_papers.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "df = df.drop_duplicates(subset='title')\n",
    "df['processed'] = df['title'] + \". \" + df['abstract']\n",
    "df['cleaned_text'] = df['abstract'].apply(preprocess)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df: 266081\n"
     ]
    }
   ],
   "source": [
    "## how do i get number of rows in df?\n",
    "num_rows = df.shape[0]\n",
    "print(\"Number of rows in df:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the input index of the paper you want to analyze and also number of top k papers to retrieve\n",
    "input_idx_of_paper = 5\n",
    "top_k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Paper: 704.0985 -   Advances in semiconductor technology are contributing to the increasing\n",
      "complexity in the design of embedded systems. Architectures with novel\n",
      "techniques such as evolvable nature and autonomous behavior have engrossed lot\n",
      "of attention. This paper demonstrates conceptually evolvable embedded systems\n",
      "can be characterized basing on acausal nature. It is noted that in acausal\n",
      "systems, future input needs to be known, here we make a mechanism such that the\n",
      "system predicts the future inputs and exhibits pseudo acausal nature. An\n",
      "embedded system that uses theoretical framework of acausality is proposed. Our\n",
      "method aims at a novel architecture that features the hardware evolability and\n",
      "autonomous behavior alongside pseudo acausality. Various aspects of this\n",
      "architecture are discussed in detail along with the limitations.\n",
      "\n",
      "\n",
      "Top Similar Papers:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>163679</th>\n",
       "      <td>2302.13451</td>\n",
       "      <td>The transformer is a fundamental building block in deep learning, and the\\nattention mechanism is the transformer's core component. Self-supervised speech\\nrepresentation learning (SSRL) represe...</td>\n",
       "      <td>cs.SD cs.CL cs.LG eess.AS</td>\n",
       "      <td>0.288698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>1302.4958</td>\n",
       "      <td>Whereas acausal Bayesian networks represent probabilistic independence,\\ncausal Bayesian networks represent causal relationships. In this paper, we\\nexamine Bayesian methods for learning both ty...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>0.255950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  \\\n",
       "163679  2302.13451   \n",
       "6057     1302.4958   \n",
       "\n",
       "                                                                                                                                                                                                       abstract  \\\n",
       "163679    The transformer is a fundamental building block in deep learning, and the\\nattention mechanism is the transformer's core component. Self-supervised speech\\nrepresentation learning (SSRL) represe...   \n",
       "6057      Whereas acausal Bayesian networks represent probabilistic independence,\\ncausal Bayesian networks represent causal relationships. In this paper, we\\nexamine Bayesian methods for learning both ty...   \n",
       "\n",
       "                       categories  similarity_score  \n",
       "163679  cs.SD cs.CL cs.LG eess.AS          0.288698  \n",
       "6057                        cs.AI          0.255950  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# TF-IDF similarity model\n",
    "# -----------------------------\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=5)\n",
    "tfidf_matrix = vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "def get_top_k_similar_papers(input_paper_idx, k=3):\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(tfidf_matrix[input_paper_idx], tfidf_matrix)\n",
    "    \n",
    "    # Exclude self and get top-k indices\n",
    "    similarity_scores = list(enumerate(similarities[0]))\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_k_indices = [i for i, _ in similarity_scores[1:k+1]]  # Skip index 0 (self)\n",
    "    \n",
    "    # Return results\n",
    "    results = df.iloc[top_k_indices][['id', 'abstract', 'categories']]\n",
    "    results['similarity_score'] = [similarity_scores[i][1] for i in range(1, k+1)]\n",
    "    return results\n",
    "\n",
    "similar_papers = get_top_k_similar_papers(input_idx_of_paper, top_k)\n",
    "print(f\"Input Paper: {df.iloc[input_idx_of_paper]['id']} - {df.iloc[input_idx_of_paper]['abstract']}\\n\")\n",
    "print(\"Top Similar Papers:\")\n",
    "similar_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d3cec3f71a4975bc54840124ba9f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# SBERT similarity model\n",
    "# -----------------------------\n",
    "\n",
    "# Step 1: Load SBERT model (MiniLM is fast & accurate)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 2: Prepare abstracts\n",
    "abstracts = df['abstract'].fillna('').tolist()\n",
    "\n",
    "# Step 3: Encode abstracts into dense vectors\n",
    "embeddings = model.encode(abstracts, show_progress_bar=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memoize_number_of_times_each_paper_is_cited():\n",
    "    for i in range(5, len(df)):\n",
    "        title = df.loc[i, 'title']\n",
    "        citation_count = get_number_of_times_a_paper_is_cited(title=title)\n",
    "        df.at[i, 'citation_count'] = citation_count\n",
    "\n",
    "memoize_number_of_times_each_paper_is_cited()\n",
    "print(df[['id', 'title', 'citation_count']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Paper: 704.0985 -   Advances in semiconductor technology are contributing to the increasing\n",
      "complexity in the design of embedded systems. Architectures with novel\n",
      "techniques such as evolvable nature and autonomous behavior have engrossed lot\n",
      "of attention. This paper demonstrates conceptually evolvable embedded systems\n",
      "can be characterized basing on acausal nature. It is noted that in acausal\n",
      "systems, future input needs to be known, here we make a mechanism such that the\n",
      "system predicts the future inputs and exhibits pseudo acausal nature. An\n",
      "embedded system that uses theoretical framework of acausality is proposed. Our\n",
      "method aims at a novel architecture that features the hardware evolability and\n",
      "autonomous behavior alongside pseudo acausality. Various aspects of this\n",
      "architecture are discussed in detail along with the limitations.\n",
      "\n",
      "\n",
      "Top Similar Papers (SBERT):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45181</th>\n",
       "      <td>1905.05248</td>\n",
       "      <td>Design Space Exploration via Answer Set Programming Modulo Theories</td>\n",
       "      <td>The design of embedded systems, that are ubiquitously used in mobile devices\\nand cars, is becoming continuously more complex such that efficient\\nsystem-level design methods are becoming crucia...</td>\n",
       "      <td>cs.AI cs.LO</td>\n",
       "      <td>0.476400</td>\n",
       "      <td>25103</td>\n",
       "      <td>0.685840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265416</th>\n",
       "      <td>cs/0403031</td>\n",
       "      <td>Concept of E-machine: How does a \"dynamical\" brain learn to process\\n  \"symbolic\" information? Part I</td>\n",
       "      <td>The human brain has many remarkable information processing characteristics\\nthat deeply puzzle scientists and engineers. Among the most important and the\\nmost intriguing of these characteristic...</td>\n",
       "      <td>cs.AI cs.LG</td>\n",
       "      <td>0.414249</td>\n",
       "      <td>7131</td>\n",
       "      <td>0.598862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225511</th>\n",
       "      <td>2406.01384</td>\n",
       "      <td>Extending Structural Causal Models for Autonomous Embodied Systems</td>\n",
       "      <td>In this work we aim to bridge the divide between autonomous embodied systems\\nand causal reasoning. Autonomous embodied systems have come to increasingly\\ninteract with humans, and in many cases...</td>\n",
       "      <td>cs.AI cs.RO cs.SE</td>\n",
       "      <td>0.421662</td>\n",
       "      <td>6287</td>\n",
       "      <td>0.598337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207072</th>\n",
       "      <td>2402.03824</td>\n",
       "      <td>A call for embodied AI</td>\n",
       "      <td>We propose Embodied AI as the next fundamental step in the pursuit of\\nArtificial General Intelligence, juxtaposing it against current AI\\nadvancements, particularly Large Language Models. We tr...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>0.422943</td>\n",
       "      <td>1592</td>\n",
       "      <td>0.544893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142413</th>\n",
       "      <td>2207.11089</td>\n",
       "      <td>Do Artificial Intelligence Systems Understand?</td>\n",
       "      <td>Are intelligent machines really intelligent? Is the underlying philosophical\\nconcept of intelligence satisfactory for describing how the present systems\\nwork? Is understanding a necessary and ...</td>\n",
       "      <td>cs.AI cs.LG</td>\n",
       "      <td>0.403156</td>\n",
       "      <td>1792</td>\n",
       "      <td>0.537691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                                                                                  title  \\\n",
       "45181   1905.05248                                    Design Space Exploration via Answer Set Programming Modulo Theories   \n",
       "265416  cs/0403031  Concept of E-machine: How does a \"dynamical\" brain learn to process\\n  \"symbolic\" information? Part I   \n",
       "225511  2406.01384                                     Extending Structural Causal Models for Autonomous Embodied Systems   \n",
       "207072  2402.03824                                                                                 A call for embodied AI   \n",
       "142413  2207.11089                                                         Do Artificial Intelligence Systems Understand?   \n",
       "\n",
       "                                                                                                                                                                                                       abstract  \\\n",
       "45181     The design of embedded systems, that are ubiquitously used in mobile devices\\nand cars, is becoming continuously more complex such that efficient\\nsystem-level design methods are becoming crucia...   \n",
       "265416    The human brain has many remarkable information processing characteristics\\nthat deeply puzzle scientists and engineers. Among the most important and the\\nmost intriguing of these characteristic...   \n",
       "225511    In this work we aim to bridge the divide between autonomous embodied systems\\nand causal reasoning. Autonomous embodied systems have come to increasingly\\ninteract with humans, and in many cases...   \n",
       "207072    We propose Embodied AI as the next fundamental step in the pursuit of\\nArtificial General Intelligence, juxtaposing it against current AI\\nadvancements, particularly Large Language Models. We tr...   \n",
       "142413    Are intelligent machines really intelligent? Is the underlying philosophical\\nconcept of intelligence satisfactory for describing how the present systems\\nwork? Is understanding a necessary and ...   \n",
       "\n",
       "               categories  similarity_score  citation_count  combined_score  \n",
       "45181         cs.AI cs.LO          0.476400           25103        0.685840  \n",
       "265416        cs.AI cs.LG          0.414249            7131        0.598862  \n",
       "225511  cs.AI cs.RO cs.SE          0.421662            6287        0.598337  \n",
       "207072              cs.AI          0.422943            1592        0.544893  \n",
       "142413        cs.AI cs.LG          0.403156            1792        0.537691  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sbert_top_n_candidates(input_paper_idx, n=50):\n",
    "    query_vec = embeddings[input_paper_idx].reshape(1, -1)\n",
    "    similarities = cosine_similarity(query_vec, embeddings).flatten()\n",
    "    \n",
    "    # Exclude the query paper\n",
    "    candidate_indices = [i for i in range(len(embeddings)) if i != input_paper_idx]\n",
    "    \n",
    "    # Sort by similarity\n",
    "    sorted_candidates = sorted(candidate_indices, key=lambda idx: similarities[idx], reverse=True)\n",
    "    \n",
    "    # Keep top-n\n",
    "    top_n_indices = sorted_candidates[:n]\n",
    "    \n",
    "    return [(idx, similarities[idx]) for idx in top_n_indices]\n",
    "\n",
    "\n",
    "\n",
    "def rerank_with_citations(top_n_candidates, alpha=0.6):\n",
    "    rows = []\n",
    "    for idx, sim_score in top_n_candidates:\n",
    "        title = df.iloc[idx]['title']\n",
    "        citation_count = get_number_of_times_a_paper_is_cited(title)  # Your custom function\n",
    "        rows.append({\n",
    "            'idx': idx,\n",
    "            'similarity_score': sim_score,\n",
    "            'citation_count': citation_count\n",
    "        })\n",
    "        \n",
    "    # Normalize citation counts via log scale\n",
    "    citation_values = [np.log1p(r['citation_count']) for r in rows]\n",
    "    max_c = max(citation_values) if citation_values else 1\n",
    "    for r, c in zip(rows, citation_values):\n",
    "        r['normalized_citations'] = c / max_c\n",
    "        \n",
    "    # Compute combined score\n",
    "    for r in rows:\n",
    "        r['combined_score'] = alpha * r['similarity_score'] + (1 - alpha) * r['normalized_citations']\n",
    "        \n",
    "    # Sort descending by combined_score\n",
    "    rows = sorted(rows, key=lambda x: x['combined_score'], reverse=True)\n",
    "    return rows\n",
    "\n",
    "def get_top_k_similar_sbert_with_citations(input_paper_idx, k=5, pool_size=100, alpha=0.6):\n",
    "    # 1) Get top-N by pure semantic similarity\n",
    "    top_n_candidates = get_sbert_top_n_candidates(input_paper_idx, n=pool_size)\n",
    "    \n",
    "    # 2) Re-rank with citations\n",
    "    reranked = rerank_with_citations(top_n_candidates, alpha=alpha)\n",
    "    \n",
    "    # 3) Build DataFrame for top-k\n",
    "    top_k = reranked[:k]\n",
    "    indices = [r['idx'] for r in top_k]\n",
    "    \n",
    "    results = df.iloc[indices][['id', 'title', 'abstract', 'categories']].copy()\n",
    "    results['similarity_score'] = [r['similarity_score'] for r in top_k]\n",
    "    results['citation_count'] = [r['citation_count'] for r in top_k]\n",
    "    results['combined_score'] = [r['combined_score'] for r in top_k]\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "top_k = 5\n",
    "similar_papers_sbert = get_top_k_similar_sbert_with_citations(input_idx_of_paper, top_k)\n",
    "\n",
    "# Display\n",
    "print(f\"Input Paper: {df.iloc[input_idx_of_paper]['id']} - {df.iloc[input_idx_of_paper]['abstract']}\\n\")\n",
    "print(\"Top Similar Papers (SBERT):\")\n",
    "similar_papers_sbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For even more precision we can use Cross encoder to re-rank the similar papers based on the input paper's abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Can the Internet cope with stress?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2047"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# title = df.iloc[28]['title']\n",
    "# print(f\"Title: {title}\")\n",
    "# get_number_of_times_a_paper_is_cited(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are trying to use a model that was created with Sentence Transformers version 4.1.0.dev0, but you're currently using version 4.0.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>cross_score</th>\n",
       "      <th>cross_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45181</th>\n",
       "      <td>1905.05248</td>\n",
       "      <td>The design of embedded systems, that are ubiquitously used in mobile devices\\nand cars, is becoming continuously more complex such that efficient\\nsystem-level design methods are becoming crucia...</td>\n",
       "      <td>-0.210489</td>\n",
       "      <td>0.447571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207072</th>\n",
       "      <td>2402.03824</td>\n",
       "      <td>We propose Embodied AI as the next fundamental step in the pursuit of\\nArtificial General Intelligence, juxtaposing it against current AI\\nadvancements, particularly Large Language Models. We tr...</td>\n",
       "      <td>-1.021541</td>\n",
       "      <td>0.264727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225511</th>\n",
       "      <td>2406.01384</td>\n",
       "      <td>In this work we aim to bridge the divide between autonomous embodied systems\\nand causal reasoning. Autonomous embodied systems have come to increasingly\\ninteract with humans, and in many cases...</td>\n",
       "      <td>-3.375798</td>\n",
       "      <td>0.033060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265416</th>\n",
       "      <td>cs/0403031</td>\n",
       "      <td>The human brain has many remarkable information processing characteristics\\nthat deeply puzzle scientists and engineers. Among the most important and the\\nmost intriguing of these characteristic...</td>\n",
       "      <td>-5.302018</td>\n",
       "      <td>0.004957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142413</th>\n",
       "      <td>2207.11089</td>\n",
       "      <td>Are intelligent machines really intelligent? Is the underlying philosophical\\nconcept of intelligence satisfactory for describing how the present systems\\nwork? Is understanding a necessary and ...</td>\n",
       "      <td>-5.323239</td>\n",
       "      <td>0.004853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  \\\n",
       "45181   1905.05248   \n",
       "207072  2402.03824   \n",
       "225511  2406.01384   \n",
       "265416  cs/0403031   \n",
       "142413  2207.11089   \n",
       "\n",
       "                                                                                                                                                                                                       abstract  \\\n",
       "45181     The design of embedded systems, that are ubiquitously used in mobile devices\\nand cars, is becoming continuously more complex such that efficient\\nsystem-level design methods are becoming crucia...   \n",
       "207072    We propose Embodied AI as the next fundamental step in the pursuit of\\nArtificial General Intelligence, juxtaposing it against current AI\\nadvancements, particularly Large Language Models. We tr...   \n",
       "225511    In this work we aim to bridge the divide between autonomous embodied systems\\nand causal reasoning. Autonomous embodied systems have come to increasingly\\ninteract with humans, and in many cases...   \n",
       "265416    The human brain has many remarkable information processing characteristics\\nthat deeply puzzle scientists and engineers. Among the most important and the\\nmost intriguing of these characteristic...   \n",
       "142413    Are intelligent machines really intelligent? Is the underlying philosophical\\nconcept of intelligence satisfactory for describing how the present systems\\nwork? Is understanding a necessary and ...   \n",
       "\n",
       "        cross_score  cross_probs  \n",
       "45181     -0.210489     0.447571  \n",
       "207072    -1.021541     0.264727  \n",
       "225511    -3.375798     0.033060  \n",
       "265416    -5.302018     0.004957  \n",
       "142413    -5.323239     0.004853  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Cross-encoder re-ranking\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "# 1. Load a pretrained cross-encoder model (can be swapped with others)\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# 2. Get top-k SBERT candidates first (e.g., top 5)\n",
    "sbert_top_k = similar_papers_sbert\n",
    "\n",
    "# 3. Prepare pairs: (query abstract, candidate abstract)\n",
    "query_abstract = df.iloc[input_idx_of_paper]['abstract']\n",
    "candidate_abstracts = sbert_top_k['abstract'].tolist()\n",
    "query_pairs = [(query_abstract, cand) for cand in candidate_abstracts]\n",
    "\n",
    "# 4. Get similarity scores from the cross-encoder\n",
    "cross_scores = cross_encoder.predict(query_pairs, convert_to_tensor=True)\n",
    "\n",
    "# 5. Rerank based on cross-encoder scores\n",
    "sbert_top_k['cross_score'] = cross_scores.cpu()\n",
    "sbert_top_k['cross_probs'] = F.sigmoid(cross_scores).cpu().numpy()\n",
    "sbert_top_k_sorted = sbert_top_k.sort_values(by='cross_probs', ascending=False).head(top_k)\n",
    "\n",
    "# 6. View top reranked results\n",
    "sbert_top_k_sorted[['id', 'abstract', 'cross_score', 'cross_probs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected paper has 25103 citations and a combined relevance score of 0.686.\n",
      "It is considered relevant because:\n",
      "Current paper talks about  'design embedded systems' ‚Üí and retrieved paper also talks about 'design embedded systems' (score: 1.000)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Keyword-based explainability\n",
    "# -----------------------------\n",
    "\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "def explain_similarity_with_keywords(query_abstract, candidate_abstract, top_n=10):\n",
    "    # Extract top-n keywords from both abstracts\n",
    "    query_keywords = [kw for kw, _ in kw_model.extract_keywords(\n",
    "        query_abstract, top_n=top_n, stop_words='english', keyphrase_ngram_range=(1, 3), use_maxsum=True, nr_candidates=top_n)]\n",
    "    candidate_keywords = [kw for kw, _ in kw_model.extract_keywords(\n",
    "        candidate_abstract, top_n=top_n, stop_words='english', keyphrase_ngram_range=(1, 3))]\n",
    "\n",
    "    query_keywords = semantically_deduplicate_keywords(query_keywords, model, similarity_threshold=0.8)\n",
    "    candidate_keywords = semantically_deduplicate_keywords(candidate_keywords, model, similarity_threshold=0.8)\n",
    "\n",
    "    matched_pairs = one_to_one_keyword_matches(query_keywords, candidate_keywords, model, threshold=0.6, top_n=5)\n",
    "\n",
    "    return matched_pairs\n",
    "\n",
    "\n",
    "def one_to_one_keyword_matches(query_keywords, candidate_keywords, model, threshold, top_n=None):\n",
    "    # Encode phrases\n",
    "    query_embs = model.encode(query_keywords, convert_to_tensor=True)\n",
    "    candidate_embs = model.encode(candidate_keywords, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    sim_matrix = util.cos_sim(query_embs, candidate_embs).cpu().numpy()\n",
    "\n",
    "    # Flatten and sort all (i, j, score) tuples\n",
    "    all_pairs = []\n",
    "    for i in range(len(query_keywords)):\n",
    "        for j in range(len(candidate_keywords)):\n",
    "            score = sim_matrix[i][j]\n",
    "            if score >= threshold:\n",
    "                all_pairs.append((i, j, score))\n",
    "\n",
    "    # Sort pairs by score in descending order\n",
    "    all_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    matched_query_indices = set()\n",
    "    matched_candidate_indices = set()\n",
    "    matched_pairs = []\n",
    "\n",
    "    # Greedy 1-to-1 matching\n",
    "    for i, j, score in all_pairs:\n",
    "        if i not in matched_query_indices and j not in matched_candidate_indices:\n",
    "            matched_pairs.append((query_keywords[i], candidate_keywords[j], round(score, 3)))\n",
    "            matched_query_indices.add(i)\n",
    "            matched_candidate_indices.add(j)\n",
    "            if top_n and len(matched_pairs) >= top_n:\n",
    "                break\n",
    "\n",
    "    return matched_pairs\n",
    "\n",
    "def semantically_deduplicate_keywords(keywords, model, similarity_threshold):\n",
    "    embeddings = model.encode(keywords, convert_to_tensor=True)\n",
    "    keep = []\n",
    "    used_indices = set()\n",
    "\n",
    "    for i in range(len(keywords)):\n",
    "        if i in used_indices:\n",
    "            continue\n",
    "        keep.append(keywords[i])\n",
    "        sims = util.cos_sim(embeddings[i], embeddings).squeeze()\n",
    "        for j in range(i + 1, len(keywords)):\n",
    "            if sims[j] > similarity_threshold:\n",
    "                used_indices.add(j)\n",
    "\n",
    "    return keep\n",
    "\n",
    "## Explainability by showing which keywords from the input paper match the candidate paper\n",
    "\n",
    "query_abs = df.iloc[input_idx_of_paper]['abstract']\n",
    "\n",
    "candidate_abs = sbert_top_k_sorted.iloc[0]['abstract']  # top match\n",
    "citation_count = sbert_top_k_sorted.iloc[0]['citation_count']\n",
    "combined_score = sbert_top_k_sorted.iloc[0]['combined_score']\n",
    "\n",
    "matched_pairs = explain_similarity_with_keywords(query_abs, candidate_abs)\n",
    "\n",
    "print(f\"Selected paper has {citation_count} citations and a combined relevance score of {combined_score:.3f}.\")\n",
    "print(\"It is considered relevant because:\")\n",
    "for q, c, score in matched_pairs:\n",
    "    print(f\"Current paper talks about  '{q}' ‚Üí and retrieved paper also talks about '{c}' (score: {score:.3f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Evaluation Metrics:\n",
      "‚Üí Average Cosine Similarity: 0.3557\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Evaluation Metrics\n",
    "# -----------------------------\n",
    "\n",
    "## Average Cosine Similarity\n",
    "\n",
    "\n",
    "def evaluate_tfidf_cosine(df, tfidf_matrix, top_k=5):\n",
    "    avg_cosine_scores = []\n",
    "    # purity_scores = []\n",
    "    categories = df['categories'].fillna(\"unknown\").tolist()\n",
    "\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        query_vec = tfidf_matrix[i]\n",
    "        sims = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "        sims[i] = -1  # exclude self\n",
    "\n",
    "        top_k_indices = sims.argsort()[::-1][:top_k]\n",
    "        avg_cosine_scores.append(np.mean(sims[top_k_indices]))\n",
    "\n",
    "        if (i > 50):\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"avg_cosine_similarity\": np.mean(avg_cosine_scores),\n",
    "    }\n",
    "\n",
    "results = evaluate_tfidf_cosine(df, tfidf_matrix, top_k=5)\n",
    "\n",
    "print(\"TF-IDF Evaluation Metrics:\")\n",
    "print(\"‚Üí Average Cosine Similarity:\", round(results[\"avg_cosine_similarity\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT Evaluation:\n",
      "‚Üí Average Cosine Similarity: 0.6486\n"
     ]
    }
   ],
   "source": [
    "def evaluate_sbert_similarity(sbert_embeddings, categories, top_k=5):\n",
    "    avg_cosine_scores = []\n",
    "    # purity_scores = []\n",
    "\n",
    "    for i in range(len(sbert_embeddings)):\n",
    "        query_vec = sbert_embeddings[i].reshape(1, -1)\n",
    "        sims = cosine_similarity(query_vec, sbert_embeddings).flatten()\n",
    "        sims[i] = -1  # exclude self\n",
    "\n",
    "        top_k_indices = sims.argsort()[::-1][:top_k]\n",
    "        avg_cosine_scores.append(np.mean(sims[top_k_indices]))\n",
    "\n",
    "        if (i > 50):\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"avg_cosine_similarity\": np.mean(avg_cosine_scores),\n",
    "        # \"category_purity\": np.mean(purity_scores)\n",
    "    }\n",
    "\n",
    "categories = df['categories'].fillna(\"unknown\").tolist()\n",
    "results = evaluate_sbert_similarity(embeddings, categories, top_k=5)\n",
    "\n",
    "print(\"SBERT Evaluation:\")\n",
    "print(\"‚Üí Average Cosine Similarity:\", round(results[\"avg_cosine_similarity\"], 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
