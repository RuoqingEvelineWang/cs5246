{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Notebook Summary: Find Semantically Similar Research Papers\n",
    "\n",
    "This notebook demonstrates a multi-method approach for retrieving semantically similar research papers using their abstracts. It includes:\n",
    "\n",
    "1. **TF-IDF Retrieval**: Traditional cosine similarity on bag-of-words representations.\n",
    "2. **SBERT-Based Semantic Retrieval**: Dense sentence embeddings using `all-MiniLM-L6-v2`.\n",
    "3. **Cross-Encoder Reranking**: Fine-grained relevance scoring using the `cross-encoder/ms-marco` model.\n",
    "4. **Explainability with KeyBERT**: Visual inspection of overlapping keywords between query and candidate abstracts.\n",
    "5. **Evaluation**: Measures retrieval effectiveness using average cosine similarity.\n",
    "\n",
    "Each retrieval method is modular, and the notebook can be easily extended for additional models, visualizations, or use cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import *\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from helper_functions import preprocess\n",
    "from keybert import KeyBERT\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "\n",
    "seed=25 #for random state\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_colwidth', 200) \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8l/vb4tv7t57rscnzp2p_mm47g00000gn/T/ipykernel_34875/2833354111.py:5: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Load and preprocess dataset\n",
    "# -----------------------------\n",
    "file_path = \"ai_ml_papers.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.dropna(subset=['title', 'abstract'])\n",
    "df = df.drop_duplicates(subset='title')\n",
    "df['processed'] = df['title'] + \". \" + df['abstract']\n",
    "df['cleaned_text'] = df['abstract'].apply(preprocess)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Paper: 704.0047 -   The intelligent acoustic emission locator is described in Part I, while Part\n",
      "II discusses blind source separation, time delay estimation and location of two\n",
      "simultaneously active continuous acoustic emission sources.\n",
      "  The location of acoustic emission on complicated aircraft frame structures is\n",
      "a difficult problem of non-destructive testing. This article describes an\n",
      "intelligent acoustic emission source locator. The intelligent locator comprises\n",
      "a sensor antenna and a general regression neural network, which solves the\n",
      "location problem based on learning from examples. Locator performance was\n",
      "tested on different test specimens. Tests have shown that the accuracy of\n",
      "location depends on sound velocity and attenuation in the specimen, the\n",
      "dimensions of the tested area, and the properties of stored data. The location\n",
      "accuracy achieved by the intelligent locator is comparable to that obtained by\n",
      "the conventional triangulation method, while the applicability of the\n",
      "intelligent locator is more general since analysis of sonic ray paths is\n",
      "avoided. This is a promising method for non-destructive testing of aircraft\n",
      "frame structures by the acoustic emission method.\n",
      "\n",
      "\n",
      "Top Similar Papers:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.005</td>\n",
       "      <td>Part I describes an intelligent acoustic emission locator, while Part II\\ndiscusses blind source separation, time delay estimation and location of two\\ncontinuous acoustic emission sources.\\n  A...</td>\n",
       "      <td>cs.NE cs.AI</td>\n",
       "      <td>0.569653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220454</th>\n",
       "      <td>2405.0234</td>\n",
       "      <td>Reducing Carbon dioxide (CO2) emission is vital at both global and national\\nlevels, given their significant role in exacerbating climate change. CO2\\nemission, stemming from a variety of indust...</td>\n",
       "      <td>stat.AP cs.LG</td>\n",
       "      <td>0.240823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  \\\n",
       "1         704.005   \n",
       "220454  2405.0234   \n",
       "\n",
       "                                                                                                                                                                                                       abstract  \\\n",
       "1         Part I describes an intelligent acoustic emission locator, while Part II\\ndiscusses blind source separation, time delay estimation and location of two\\ncontinuous acoustic emission sources.\\n  A...   \n",
       "220454    Reducing Carbon dioxide (CO2) emission is vital at both global and national\\nlevels, given their significant role in exacerbating climate change. CO2\\nemission, stemming from a variety of indust...   \n",
       "\n",
       "           categories  similarity_score  \n",
       "1         cs.NE cs.AI          0.569653  \n",
       "220454  stat.AP cs.LG          0.240823  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# TF-IDF similarity model\n",
    "# -----------------------------\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=5)\n",
    "tfidf_matrix = vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "def get_top_k_similar_papers(input_paper_idx, k=3):\n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(tfidf_matrix[input_paper_idx], tfidf_matrix)\n",
    "    \n",
    "    # Exclude self and get top-k indices\n",
    "    similarity_scores = list(enumerate(similarities[0]))\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_k_indices = [i for i, _ in similarity_scores[1:k+1]]  # Skip index 0 (self)\n",
    "    \n",
    "    # Return results\n",
    "    results = df.iloc[top_k_indices][['id', 'abstract', 'categories']]\n",
    "    results['similarity_score'] = [similarity_scores[i][1] for i in range(1, k+1)]\n",
    "    return results\n",
    "\n",
    "input_idx = 0\n",
    "top_k = 2\n",
    "similar_papers = get_top_k_similar_papers(input_idx, top_k)\n",
    "print(f\"Input Paper: {df.iloc[input_idx]['id']} - {df.iloc[input_idx]['abstract']}\\n\")\n",
    "print(\"Top Similar Papers:\")\n",
    "similar_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# SBERT similarity model\n",
    "# -----------------------------\n",
    "\n",
    "# Step 1: Load SBERT model (MiniLM is fast & accurate)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 2: Prepare abstracts\n",
    "abstracts = df['abstract'].fillna('').tolist()\n",
    "\n",
    "# Step 3: Encode abstracts into dense vectors\n",
    "embeddings = model.encode(abstracts, show_progress_bar=True, batch_size=64)\n",
    "\n",
    "# Paper index that needs to be summarized\n",
    "input_idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Paper: 704.0304 -   This paper discusses the benefits of describing the world as information,\n",
      "especially in the study of the evolution of life and cognition. Traditional\n",
      "studies encounter problems because it is difficult to describe life and\n",
      "cognition in terms of matter and energy, since their laws are valid only at the\n",
      "physical scale. However, if matter and energy, as well as life and cognition,\n",
      "are described in terms of information, evolution can be described consistently\n",
      "as information becoming more complex.\n",
      "  The paper presents eight tentative laws of information, valid at multiple\n",
      "scales, which are generalizations of Darwinian, cybernetic, thermodynamic,\n",
      "psychological, philosophical, and complexity principles. These are further used\n",
      "to discuss the notions of life, cognition and their evolution.\n",
      "\n",
      "\n",
      "Top Similar Papers (SBERT):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7914</th>\n",
       "      <td>1311.0413</td>\n",
       "      <td>Nature can be seen as informational structure with computational dynamics\\n(info-computationalism), where an (info-computational) agent is needed for the\\npotential information of the world to a...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>0.704437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100692</th>\n",
       "      <td>2105.03216</td>\n",
       "      <td>Even when concepts similar to emergence have been used since antiquity, we\\nlack an agreed definition. However, emergence has been identified as one of the\\nmain features of complex systems. Mos...</td>\n",
       "      <td>physics.gen-ph cs.AI</td>\n",
       "      <td>0.647915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8572</th>\n",
       "      <td>1401.4942</td>\n",
       "      <td>This paper addresses the open question formulated as: Which levels of\\nabstraction are appropriate in the synthetic modelling of life and cognition?\\nwithin the framework of info-computational c...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>0.616080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>912.4649</td>\n",
       "      <td>In this review we integrate results of long term experimental study on ant\\n\"language\" and intelligence which were fully based on fundamental ideas of\\nInformation Theory, such as the Shannon en...</td>\n",
       "      <td>cs.IT cs.AI math.IT nlin.AO</td>\n",
       "      <td>0.596172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15809</th>\n",
       "      <td>1605.05676</td>\n",
       "      <td>We present some arguments why existing methods for representing agents fall\\nshort in applications crucial to artificial life. Using a thought experiment\\ninvolving a fictitious dynamical system...</td>\n",
       "      <td>cs.AI cs.SI</td>\n",
       "      <td>0.594813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  \\\n",
       "7914     1311.0413   \n",
       "100692  2105.03216   \n",
       "8572     1401.4942   \n",
       "1142      912.4649   \n",
       "15809   1605.05676   \n",
       "\n",
       "                                                                                                                                                                                                       abstract  \\\n",
       "7914      Nature can be seen as informational structure with computational dynamics\\n(info-computationalism), where an (info-computational) agent is needed for the\\npotential information of the world to a...   \n",
       "100692    Even when concepts similar to emergence have been used since antiquity, we\\nlack an agreed definition. However, emergence has been identified as one of the\\nmain features of complex systems. Mos...   \n",
       "8572      This paper addresses the open question formulated as: Which levels of\\nabstraction are appropriate in the synthetic modelling of life and cognition?\\nwithin the framework of info-computational c...   \n",
       "1142      In this review we integrate results of long term experimental study on ant\\n\"language\" and intelligence which were fully based on fundamental ideas of\\nInformation Theory, such as the Shannon en...   \n",
       "15809     We present some arguments why existing methods for representing agents fall\\nshort in applications crucial to artificial life. Using a thought experiment\\ninvolving a fictitious dynamical system...   \n",
       "\n",
       "                         categories  similarity_score  \n",
       "7914                          cs.AI          0.704437  \n",
       "100692         physics.gen-ph cs.AI          0.647915  \n",
       "8572                          cs.AI          0.616080  \n",
       "1142    cs.IT cs.AI math.IT nlin.AO          0.596172  \n",
       "15809                   cs.AI cs.SI          0.594813  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 4: Define function to get top-k similar papers\n",
    "def get_top_k_similar_sbert(input_paper_idx, k=5):\n",
    "    query_vec = embeddings[input_paper_idx].reshape(1, -1)\n",
    "    similarities = cosine_similarity(query_vec, embeddings).flatten()\n",
    "    \n",
    "    # Get top-k indices excluding the query paper itself\n",
    "    top_k_indices = similarities.argsort()[::-1][1:k+1]\n",
    "    \n",
    "    results = df.iloc[top_k_indices][['id', 'abstract', 'categories']].copy()\n",
    "    results['similarity_score'] = similarities[top_k_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "top_k = 5\n",
    "similar_papers_sbert = get_top_k_similar_sbert(input_idx, top_k)\n",
    "\n",
    "# Display\n",
    "print(f\"Input Paper: {df.iloc[input_idx]['id']} - {df.iloc[input_idx]['abstract']}\\n\")\n",
    "print(\"Top Similar Papers (SBERT):\")\n",
    "similar_papers_sbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For even more precision we can use Cross encoder to re-rank the similar papers based on the input paper's abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>cross_score</th>\n",
       "      <th>cross_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8572</th>\n",
       "      <td>1401.4942</td>\n",
       "      <td>This paper addresses the open question formulated as: Which levels of\\nabstraction are appropriate in the synthetic modelling of life and cognition?\\nwithin the framework of info-computational c...</td>\n",
       "      <td>0.710034</td>\n",
       "      <td>0.670409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100692</th>\n",
       "      <td>2105.03216</td>\n",
       "      <td>Even when concepts similar to emergence have been used since antiquity, we\\nlack an agreed definition. However, emergence has been identified as one of the\\nmain features of complex systems. Mos...</td>\n",
       "      <td>0.439256</td>\n",
       "      <td>0.608082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7914</th>\n",
       "      <td>1311.0413</td>\n",
       "      <td>Nature can be seen as informational structure with computational dynamics\\n(info-computationalism), where an (info-computational) agent is needed for the\\npotential information of the world to a...</td>\n",
       "      <td>-1.049732</td>\n",
       "      <td>0.259277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>912.4649</td>\n",
       "      <td>In this review we integrate results of long term experimental study on ant\\n\"language\" and intelligence which were fully based on fundamental ideas of\\nInformation Theory, such as the Shannon en...</td>\n",
       "      <td>-3.495477</td>\n",
       "      <td>0.029441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15809</th>\n",
       "      <td>1605.05676</td>\n",
       "      <td>We present some arguments why existing methods for representing agents fall\\nshort in applications crucial to artificial life. Using a thought experiment\\ninvolving a fictitious dynamical system...</td>\n",
       "      <td>-3.779365</td>\n",
       "      <td>0.022327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  \\\n",
       "8572     1401.4942   \n",
       "100692  2105.03216   \n",
       "7914     1311.0413   \n",
       "1142      912.4649   \n",
       "15809   1605.05676   \n",
       "\n",
       "                                                                                                                                                                                                       abstract  \\\n",
       "8572      This paper addresses the open question formulated as: Which levels of\\nabstraction are appropriate in the synthetic modelling of life and cognition?\\nwithin the framework of info-computational c...   \n",
       "100692    Even when concepts similar to emergence have been used since antiquity, we\\nlack an agreed definition. However, emergence has been identified as one of the\\nmain features of complex systems. Mos...   \n",
       "7914      Nature can be seen as informational structure with computational dynamics\\n(info-computationalism), where an (info-computational) agent is needed for the\\npotential information of the world to a...   \n",
       "1142      In this review we integrate results of long term experimental study on ant\\n\"language\" and intelligence which were fully based on fundamental ideas of\\nInformation Theory, such as the Shannon en...   \n",
       "15809     We present some arguments why existing methods for representing agents fall\\nshort in applications crucial to artificial life. Using a thought experiment\\ninvolving a fictitious dynamical system...   \n",
       "\n",
       "        cross_score  cross_probs  \n",
       "8572       0.710034     0.670409  \n",
       "100692     0.439256     0.608082  \n",
       "7914      -1.049732     0.259277  \n",
       "1142      -3.495477     0.029441  \n",
       "15809     -3.779365     0.022327  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Cross-encoder re-ranking\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "# 1. Load a pretrained cross-encoder model (can be swapped with others)\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# 2. Get top-k SBERT candidates first (e.g., top 5)\n",
    "sbert_top_k = get_top_k_similar_sbert(input_idx, k=5)\n",
    "\n",
    "# 3. Prepare pairs: (query abstract, candidate abstract)\n",
    "query_abstract = df.iloc[input_idx]['abstract']\n",
    "candidate_abstracts = sbert_top_k['abstract'].tolist()\n",
    "query_pairs = [(query_abstract, cand) for cand in candidate_abstracts]\n",
    "\n",
    "# 4. Get similarity scores from the cross-encoder\n",
    "cross_scores = cross_encoder.predict(query_pairs, convert_to_tensor=True)\n",
    "\n",
    "# 5. Rerank based on cross-encoder scores\n",
    "sbert_top_k['cross_score'] = cross_scores.cpu()\n",
    "sbert_top_k['cross_probs'] = F.sigmoid(cross_scores).cpu().numpy()\n",
    "sbert_top_k_sorted = sbert_top_k.sort_values(by='cross_probs', ascending=False).head(top_k)\n",
    "\n",
    "# 6. View top reranked results\n",
    "sbert_top_k_sorted[['id', 'abstract', 'cross_score', 'cross_probs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Keyword Pairs (Query ‚Üí Candidate):\n",
      "  evolution       ‚Üí organisms       (score: 0.553)\n",
      "  cognition       ‚Üí cognition       (score: 1.000)\n",
      "  cognition       ‚Üí cognizing       (score: 0.612)\n",
      "  information     ‚Üí informational   (score: 0.753)\n",
      "  darwinian       ‚Üí organisms       (score: 0.520)\n",
      "  complexity      ‚Üí computational   (score: 0.597)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Keyword-based explainability\n",
    "# -----------------------------\n",
    "\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "def explain_similarity_with_keywords(query_abstract, candidate_abstract, top_n=10):\n",
    "    # Extract top-n keywords\n",
    "    query_keywords = [kw for kw, _ in kw_model.extract_keywords(query_abstract, top_n=top_n, stop_words='english')]\n",
    "    candidate_keywords = [kw for kw, _ in kw_model.extract_keywords(candidate_abstract, top_n=top_n, stop_words='english')]\n",
    "    \n",
    "    query_embs = model.encode(query_keywords, convert_to_tensor=True)\n",
    "    candidate_embs = model.encode(candidate_keywords, convert_to_tensor=True)\n",
    "\n",
    "    # Just keep the keyword strings (ignore scores)\n",
    "    # query_kw_set = set([kw for kw, _ in query_keywords])\n",
    "    # candidate_kw_set = set([kw for kw, _ in candidate_keywords])\n",
    "\n",
    "    sim_matrix = util.cos_sim(query_embs, candidate_embs)\n",
    "    \n",
    "    # Find overlapping keywords\n",
    "    similarity_threshold = 0.5\n",
    "    matched_pairs = []\n",
    "    for i, q_kw in enumerate(query_keywords):\n",
    "        for j, c_kw in enumerate(candidate_keywords):\n",
    "            if sim_matrix[i][j] >= similarity_threshold:\n",
    "                matched_pairs.append((q_kw, c_kw, round(float(sim_matrix[i][j]), 3)))\n",
    "    \n",
    "    return {\n",
    "        'matched_keywords_pairs': matched_pairs,\n",
    "        'num_matches': len(matched_pairs)\n",
    "    }\n",
    "\n",
    "## Explainability by showing which keywords from the input paper match the candidate paper\n",
    "\n",
    "query_abs = df.iloc[input_idx]['abstract']\n",
    "\n",
    "candidate_abs = sbert_top_k_sorted.iloc[0]['abstract']  # top match\n",
    "\n",
    "explanation = explain_similarity_with_keywords(query_abs, candidate_abs)\n",
    "\n",
    "matched_pairs = explanation['matched_keywords_pairs']\n",
    "# print(\"Query Keywords:\", explanation['query_keywords'])\n",
    "# print(\"Candidate Keywords:\", explanation['candidate_keywords'])\n",
    "\n",
    "print(\"Matched Keyword Pairs (Query ‚Üí Candidate):\")\n",
    "for q, c, score in matched_pairs:\n",
    "    print(f\"  {q:15} ‚Üí {c:15} (score: {score:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_phrases(text):\n",
    "    doc = nlp(text)\n",
    "    return [chunk.text.lower() for chunk in doc.noun_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_chunks = sent_tokenize(query_abs)\n",
    "candidate_chunks = sent_tokenize(candidate_abs)\n",
    "\n",
    "query_embs = model.encode(query_chunks, convert_to_tensor=True)\n",
    "candidate_embs = model.encode(candidate_chunks, convert_to_tensor=True)\n",
    "\n",
    "sim_matrix = util.cos_sim(query_embs, candidate_embs)\n",
    "\n",
    "aligned_pairs = []\n",
    "threshold = 0.5\n",
    "for i, query_sent in enumerate(query_chunks):\n",
    "    for j, cand_sent in enumerate(candidate_chunks):\n",
    "        score = sim_matrix[i][j]\n",
    "        if score >= threshold:\n",
    "            aligned_pairs.append((query_sent, cand_sent, float(score)))\n",
    "\n",
    "aligned_pairs = sorted(aligned_pairs, key=lambda x: x[2], reverse=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This candidate paper is similar to your query because:\n",
      "‚Ä¢ Both mention **life** (semantic similarity: 1.0)\n",
      "‚Ä¢ Both mention **cognition** (semantic similarity: 1.0)\n",
      "‚Ä¢ Both mention **  this paper** (semantic similarity: 1.0)\n"
     ]
    }
   ],
   "source": [
    "concept_pairs = []\n",
    "for q_sent, c_sent, score in aligned_pairs:\n",
    "    q_concepts = extract_phrases(q_sent)\n",
    "    c_concepts = extract_phrases(c_sent)\n",
    "\n",
    "    for q in q_concepts:\n",
    "        for c in c_concepts:\n",
    "            # Compute semantic similarity between q and c\n",
    "            q_emb = model.encode(q, convert_to_tensor=True)\n",
    "            c_emb = model.encode(c, convert_to_tensor=True)\n",
    "            sim = util.cos_sim(q_emb, c_emb).item()\n",
    "\n",
    "            if sim >= 0.7:\n",
    "                concept_pairs.append((q, c, round(sim, 3)))\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "merged_concepts = defaultdict(float)\n",
    "for q, c, sim in concept_pairs:\n",
    "    label = q if q == c else f\"{q} / {c}\"\n",
    "    merged_concepts[label] = max(merged_concepts[label], sim)\n",
    "\n",
    "final_concepts = sorted(merged_concepts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "print(\"This candidate paper is similar to your query because:\")\n",
    "for concept, sim in final_concepts:\n",
    "    print(f\"‚Ä¢ Both mention **{concept}** (semantic similarity: {sim})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Evaluation Metrics:\n",
      "‚Üí Average Cosine Similarity: 0.3557\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Evaluation Metrics\n",
    "# -----------------------------\n",
    "\n",
    "## Average Cosine Similarity\n",
    "\n",
    "\n",
    "def evaluate_tfidf_cosine(df, tfidf_matrix, top_k=5):\n",
    "    avg_cosine_scores = []\n",
    "    # purity_scores = []\n",
    "    categories = df['categories'].fillna(\"unknown\").tolist()\n",
    "\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        query_vec = tfidf_matrix[i]\n",
    "        sims = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "        sims[i] = -1  # exclude self\n",
    "\n",
    "        top_k_indices = sims.argsort()[::-1][:top_k]\n",
    "        avg_cosine_scores.append(np.mean(sims[top_k_indices]))\n",
    "\n",
    "        if (i > 50):\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"avg_cosine_similarity\": np.mean(avg_cosine_scores),\n",
    "        # \"category_purity\": np.mean(purity_scores)\n",
    "    }\n",
    "\n",
    "results = evaluate_tfidf_cosine(df, tfidf_matrix, top_k=5)\n",
    "\n",
    "print(\"TF-IDF Evaluation Metrics:\")\n",
    "print(\"‚Üí Average Cosine Similarity:\", round(results[\"avg_cosine_similarity\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT Evaluation:\n",
      "‚Üí Average Cosine Similarity: 0.6486\n"
     ]
    }
   ],
   "source": [
    "def evaluate_sbert_similarity(sbert_embeddings, categories, top_k=5):\n",
    "    avg_cosine_scores = []\n",
    "    # purity_scores = []\n",
    "\n",
    "    for i in range(len(sbert_embeddings)):\n",
    "        query_vec = sbert_embeddings[i].reshape(1, -1)\n",
    "        sims = cosine_similarity(query_vec, sbert_embeddings).flatten()\n",
    "        sims[i] = -1  # exclude self\n",
    "\n",
    "        top_k_indices = sims.argsort()[::-1][:top_k]\n",
    "        avg_cosine_scores.append(np.mean(sims[top_k_indices]))\n",
    "\n",
    "        # query_cat = categories[i]\n",
    "        # match_count = sum([query_cat in categories[j] for j in top_k_indices])\n",
    "        # purity_scores.append(match_count / top_k)\n",
    "\n",
    "        if (i > 50):\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"avg_cosine_similarity\": np.mean(avg_cosine_scores),\n",
    "        # \"category_purity\": np.mean(purity_scores)\n",
    "    }\n",
    "\n",
    "categories = df['categories'].fillna(\"unknown\").tolist()\n",
    "results = evaluate_sbert_similarity(embeddings, categories, top_k=5)\n",
    "\n",
    "print(\"SBERT Evaluation:\")\n",
    "print(\"‚Üí Average Cosine Similarity:\", round(results[\"avg_cosine_similarity\"], 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
