{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kv/h86sly590pv79w7ckm213kg80000gq/T/ipykernel_83059/2969752437.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('ai_ml_papers.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('ai_ml_papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_pdf import extract_pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ordinal regression is an important type of learning, which has properties of\n",
      "both classification and regression. Here we describe a simple and effective\n",
      "approach to adapt a traditional neural network to learn ordinal categories. Our\n",
      "approach is a generalization of the perceptron method for ordinal regression.\n",
      "On several benchmark datasets, our method (NNRank) outperforms a neural network\n",
      "classification method. Compared with the ordinal regression methods using\n",
      "Gaussian processes and support vector machines, NNRank achieves comparable\n",
      "performance. Moreover, NNRank has the advantages of traditional neural\n",
      "networks: learning in both online and batch modes, handling very large training\n",
      "datasets, and making rapid predictions. These features make NNRank a useful and\n",
      "complementary tool for large-scale data processing tasks such as information\n",
      "retrieval, web page ranking, collaborative filtering, and protein ranking in\n",
      "Bioinformatics.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[7]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_pdf_text(df.iloc[7]['id'])\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp311-cp311-macosx_10_9_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp311-cp311-macosx_10_9_x86_64.whl (26 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-macosx_10_9_x86_64.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m895.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-macosx_10_9_x86_64.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.0/133.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp311-cp311-macosx_10_9_x86_64.whl (839 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m839.3/839.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-macosx_10_9_x86_64.whl (635 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.9/635.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.12.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp311-cp311-macosx_10_9_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-macosx_10_9_x86_64.whl (192 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.7/192.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
      "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.15.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, shellingham, pydantic-core, murmurhash, mdurl, marisa-trie, cloudpathlib, click, catalogue, blis, annotated-types, srsly, pydantic, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 click-8.1.8 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 preshed-3.0.9 pydantic-2.10.6 pydantic-core-2.27.2 rich-13.9.4 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.66.5)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install spacy\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "!pip3 install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this if not certified: /Applications/Python\\ 3.11/Install\\ Certificates.command\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def is_meaningful_title(title):\n",
    "    doc = nlp(title)\n",
    "\n",
    "    #must contain at least one valid word\n",
    "    valid_words = [token.text for token in doc if token.is_alpha and wordnet.synsets(token.text)]\n",
    "    if not valid_words:\n",
    "        return False\n",
    "\n",
    "    #must contain a noun or a verb (common in section titles)\n",
    "    if not any(token.pos_ in {\"PROPN\", \"NOUN\", \"VERB\"} for token in doc):\n",
    "        return False\n",
    "\n",
    "    #should not be just a function word\n",
    "    if all(token.pos_ in {\"DET\", \"ADP\", \"CCONJ\"} for token in doc):\n",
    "        return False\n",
    "\n",
    "    #ensure numbering follows a common pattern\n",
    "    if doc[0].is_digit and len(doc) > 1 and doc[1].pos_ in {\"DET\", \"ADV\"}:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sections(text):\n",
    "    \"\"\"Extracts section titles and their positions from text.\"\"\"\n",
    "    section_patterns = [\n",
    "        r'^(\\d+\\.?\\s+[A-Z][A-Za-z ]+)',   # 1. Title / 1 Title\n",
    "        r'^(\\d+\\.\\d+\\.?\\s+[A-Z][A-Za-z ]+)', # 1.1 Title\n",
    "        r'^(I{1,3}|IV|V{1,3}|VI{1,3})\\.\\s+[A-Z][A-Za-z ]+',  # III. Title\n",
    "        r'^[A-Z]\\.\\s+[A-Z][A-Za-z ]+',  # A. Title\n",
    "        r'^[A-Z][A-Z ]+$'  # INTRODUCTION\n",
    "    ]\n",
    "\n",
    "    keyword_sections = {\"INTRODUCTION\", \"METHOD\", \"EXPERIMENT\", \"RESULTS\", \"DISCUSSION\", \"CONCLUSION\", \"REFERENCES\", \"ACKNOWLEDGEMENTS\", \"BIBLIOGRAPHY\"}\n",
    "\n",
    "    numbering_patterns = [\n",
    "        r'^\\d+(\\.\\d+)*\\.?\\s',  # 1, 1.1, 1.2.3\n",
    "        r'^(I{1,3}|IV|V{1,3}|VI{1,3})\\.\\s',  # I. II. III.\n",
    "        r'^[A-Z]\\.\\s'  # A. B. C.\n",
    "    ]\n",
    "\n",
    "    sections = []\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if any(re.match(pattern, line) for pattern in section_patterns) and len(line.split(' ')) < 10 or line.upper() in keyword_sections:\n",
    "            if ('references' in line.lower() or 'bibliography' in line.lower() or 'acknowledgement' in line.lower()):\n",
    "                break\n",
    "            sections.append((i, line))\n",
    "\n",
    "    #if a section name doesn't make sense, remove it\n",
    "    sections = [sec for sec in sections if is_meaningful_title(sec[1])]\n",
    "\n",
    "    #if there is an existing numbering pattern, remove other titles as noise\n",
    "    numbering_pattern_count = 0\n",
    "    for section in sections:\n",
    "        for pattern in numbering_patterns:\n",
    "            if re.match(pattern, section[1]):\n",
    "                numbering_pattern_count += 1\n",
    "    if numbering_pattern_count > 5:\n",
    "        for section in sections[:]:\n",
    "            matched = False\n",
    "\n",
    "            for pattern in numbering_patterns:\n",
    "                if re.match(pattern, section[1]):\n",
    "                    matched = True\n",
    "            if not matched:\n",
    "                sections.remove(section)\n",
    "\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, '1. Introduction')\n",
      "(121, '2. Method')\n",
      "(122, '2.1. Formulation')\n",
      "(167, '2.2. Learning')\n",
      "(266, '2.3. Prediction')\n",
      "(273, '3. Experiments and Results')\n",
      "(274, '3.1. Benchmark Data and Evaluation Metric')\n",
      "(313, '3.2. Comparison with Neural Network')\n",
      "(335, '3.3. Comparison with Gaussian Processes and')\n",
      "(353, '4. Discussion and Future Work')\n"
     ]
    }
   ],
   "source": [
    "sections = extract_sections(text)\n",
    "for section in sections:\n",
    "    print(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_numbering(title):\n",
    "    if match := re.match(r'^(I{1,3}|IV|V{1,3}|VI{1,3})\\.\\s', title):  #roman numerals\n",
    "        return (\"roman\", match.group(0).strip())\n",
    "    elif match := re.match(r'^\\d+(\\.\\d+)*\\.?\\s', title):  #numeric (1, 1.1, etc.)\n",
    "        value = match.group(0).strip()\n",
    "        return (\"numeric\" + str(value.rstrip('.').count('.')), value)\n",
    "    elif match := re.match(r'^[A-Z]\\.\\s', title):  #alphabetic (A., B., etc.)\n",
    "        return (\"alpha\", match.group(0).strip())\n",
    "    return (None, None)  #no numbering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume all section names have numbering\n",
    "def construct_hierarchy_with_numbering(sections):\n",
    "    prev_type = None\n",
    "\n",
    "    numbering_to_prefix = {}\n",
    "\n",
    "    for _, title in sections:\n",
    "        type, value = detect_numbering(title)\n",
    "        if prev_type is None:\n",
    "            print(title)\n",
    "            numbering_to_prefix[type] = ''\n",
    "            prev_type = type\n",
    "            continue\n",
    "        if type not in numbering_to_prefix:\n",
    "            if numbering_to_prefix[prev_type] == '':\n",
    "                numbering_to_prefix[type] = '  └─── '\n",
    "            else:\n",
    "                numbering_to_prefix[type] = '  ' + numbering_to_prefix[prev_type]\n",
    "        print(numbering_to_prefix[type] + title)\n",
    "        prev_type = type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Introduction\n",
      "2. Method\n",
      "  └─── 2.1. Formulation\n",
      "  └─── 2.2. Learning\n",
      "  └─── 2.3. Prediction\n",
      "3. Experiments and Results\n",
      "  └─── 3.1. Benchmark Data and Evaluation Metric\n",
      "  └─── 3.2. Comparison with Neural Network\n",
      "  └─── 3.3. Comparison with Gaussian Processes and\n",
      "4. Discussion and Future Work\n"
     ]
    }
   ],
   "source": [
    "construct_hierarchy_with_numbering(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def is_valid_word(word):\n",
    "    doc = nlp(word)\n",
    "    lemma = doc[0].lemma_\n",
    "    return word in nlp.vocab or lemma in nlp.vocab\n",
    "\n",
    "def fix_hyphenation(text):\n",
    "    #find all hyphenated words using regex\n",
    "    matches = list(re.finditer(r'(\\w+)-(\\w+)', text))\n",
    "    \n",
    "    #process matches in reverse order (to avoid index shifting issues)\n",
    "    for match in reversed(matches):\n",
    "        word1, word2 = match.groups()\n",
    "        joined_word = word1 + word2\n",
    "        \n",
    "        if is_valid_word(joined_word):\n",
    "            #replace only if the joined word is valid\n",
    "            start, end = match.span()\n",
    "            text = text[:start] + joined_word + text[end:]\n",
    "        else:\n",
    "            print(joined_word)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from Eka's code to cover some corner cases\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    #lowercase and remove URLs/special characters\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    #text = re.sub(r'[^a-zA-Z\\s]', '', text).strip()\n",
    "    \n",
    "    #Removing Extra Spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'-\\s', '-', text)\n",
    "    \n",
    "    #Joining wordssplit by -\n",
    "    text = fix_hyphenation(text)\n",
    "    \n",
    "    #Remove citations and references\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)  # Removes [12]\n",
    "    text = re.sub(r'\\([A-Z][^)]*,\\s\\d{4}[a-z]?\\)', '', text)  # Removes (Smith et al., 2020)\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
    "\n",
    "    # Tokenize and lemmatize\n",
    "    #tokens = word_tokenize(text)\n",
    "    #tokens = [word.lower() if not word.isupper() else word for word in tokens]\n",
    "    #tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_texts(text_lines, section_list):\n",
    "    section_texts = {}\n",
    "    \n",
    "    for i in range(len(section_list) - 1):\n",
    "        start_line = section_list[i][0] + 1\n",
    "        section_name = section_list[i][1]\n",
    "        end_line = section_list[i + 1][0] - 1\n",
    "\n",
    "        if start_line <= end_line:\n",
    "            section_texts[section_name] = \" \".join(text_lines[start_line:end_line + 1])\n",
    "        else:\n",
    "            section_texts[section_name] = \"\"\n",
    "\n",
    "    last_section_name = section_list[-1][1]\n",
    "    last_section_start = section_list[-1][0]\n",
    "    last_section_text = \" \".join(text_lines[last_section_start:])\n",
    "    last_section_text = last_section_text.split(\"\\nReferences\\n\")[0]\n",
    "    last_section_text = last_section_text.split(\"\\nBibliography\\n\")[0]\n",
    "    section_texts[last_section_name] = last_section_text\n",
    "\n",
    "    return section_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_texts = get_section_texts(text.split('\\n'), sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Introduction: Ordinal regression (or ranking learning) is an important supervised problem of learning a ranking or ordering on instances, which has the property of both classification and metric regression. The learning task of ordinal regression is to assign data points into a set of finite ordered categories. For example, a teacher rates students’ performance using A, B, C, D, and E (A > B > C > D > E) (Chu & Ghahramani, 2005a). Ordinal regression is different from classification due to the order of categories. In contrast to metric regression, the response variables (categories) in ordinal regression is discrete and finite. The research of ordinal regression dated back to the ordinal statistics methods in 1980s (McCullagh, 1980; McCullagh & Nelder, 1983) and machine learning research in 1990s (Caruana et al., 1996; Herbrich et al., 1998; Cohen et al., 1999). It has attracted the considerable attention in recent years due to its potential applications in many data-intensive domains such as information retrieval (Herbrich et al., 1998), web page ranking (Joachims, 2002), collaborative filtering (Goldberg et al., 1992; Basilico & Hofmann, 2004; Yu et al., 2006), image retrieval (Wu et al., 2003), and protein ranking (Cheng & Baldi, 2006) in Bioinformatics. A number of machine learning methods have been developed or redesigned to address ordinal regression problem (Rajaram et al., 2003), including perceptron (Crammer & Singer, 2002) and its kernelized generalization (Basilico & Hofmann, 2004), neural network with gradient descent (Caruana et al., 1996; Burges et al., 2005), Gaussian process (Chu & Ghahramani, 2005b; Chu & Ghahramani, 2005a; Schwaighofer et al., 2005), large margin classifier (or support vector machine) (Herbrich et al., 1999; Herbrich et al., 2000; Joachims, 2002; Shashua & Levin, 2003; Chu & Keerthi, 2005; Aiolli & Sperduti, 2004; Chu & Keerthi, 2007), k-partite classifier (Agarwal & Roth, 2005), boosting algorithm (Freund et al., 2003; Dekel et al., 2002), constraint classification (Har-Peled et al., 2002), regression trees (Kramer et al., 2001), Naive Bayes (Zhang et al., 2005), Bayesian hierarchical experts (Paquet et al., 2005), binary classification approach (Frank & Hall, 2001; Li & Lin, 2006) that decomposes the original ordinal regression problem into a set of binary classifications, and the optimization of nonsmooth cost functions (Burges et al., 2006). Most of these methods can be roughly classified into two categories: pairwise constraint approach (Herbrich et al., 2000; Joachims, 2002; Dekel et al., 2004; Burges et al., 2005) and multi-threshold approach (Crammer & Singer, 2002; Shashua & Levin, 2003; Chu & Ghahramani, 2005a). The former is to convert the full ranking relation into pairwise order constraints. The latter tries to learn multiple thresholds to divide data arXiv:0704.1028v1 [cs.LG] 8 Apr 2007 A Neural Network Approach to Ordinal Regression into ordinal categories. Multi-threshold approaches also can be unified under the general, extended binary classification framework (Li & Lin, 2006). The ordinal regression methods have different advantages and disadvantages. Prank (Crammer & Singer, 2002), a perceptron approach that generalizes the binary perceptron algorithm to the ordinal multi-class situation, is a fast online algorithm. However, like a standard perceptron method, its accuracy suffers when dealing with non-linear data, while a quadratic kernel version of Prank greatly relieves this problem. One class of accurate large-margin classifier approaches (Herbrich et al., 2000; Joachims, 2002) convert the ordinal relations into O(n 2 ) (n: the number of data points) pairwise ranking constraints for the structural risk minimization (Vapnik, 1995; Schoelkopf & Smola, 2002). Thus, it can not be applied to medium size datasets (> 10,000 data points), without discarding some pairwise preference relations. It may also overfit noise due to incomparable pairs. The other class of powerful large-margin classifier methods (Shashua & Levin, 2003; Chu & Keerthi, 2005) generalize the support vector formulation for ordinal regression by finding K − 1 thresholds on the real line that divide data into K ordered categories. The size of this optimization problem is linear in the number of training examples. However, like support vector machine used for classification, the prediction speed is slow when the solution is not sparse, which makes it not appropriate for time-critical tasks. Similarly, another state-of-the-art approach, Gaussian process method (Chu & Ghahramani, 2005a), also has the difficulty of handling large training datasets and the problem of slow prediction speed in some situations. Here we describe a new neural network approach for ordinal regression that has the advantages of neural network learning: learning in both online and batch mode, training on very large dataset (Burges et al., 2005), handling non-linear data, good performance, and rapid prediction. Our method can be considered a generalization of the perceptron learning (Crammer & Singer, 2002) into multi-layer perceptrons (neural network) for ordinal regression. Our method is also related to the classic generalized linear models (e.g., cumulative logit model) for ordinal regression (McCullagh, 1980). Unlike the neural network method (Burges et al., 2005) trained on pairs of examples to learn pairwise order relations, our method works on individual data points and uses multiple output nodes to estimate the probabilities of ordinal categories. Thus, our method falls into the category of multi-threshold approach. The learning of our method proceeds similarly as traditional neural networks using back-propagation (Rumelhart et al., 1986). On the same benchmark datasets, our method yields the performance better than the standard classification neural networks and comparable to the state-ofthe-art methods using support vector machines and Gaussian processes. In addition, our method can learn on very large datasets and make rapid predictions.\n",
      "2. Method: \n",
      "2.1. Formulation: Let D represent an ordinal regression dataset consisting of n data points (x, y) , where x ∈ Rd is an input feature vector and y is its ordinal category from a finite set Y . Without loss of generality, we assume that Y = 1, 2, ..., K with ”<” as order relation. For a standard classification neural network without considering the order of categories, the goal is to predict the probability of a data point x belonging to one category k (y = k). The input is x and the target of encoding the category k is a vector t = (0, ..., 0, 1, 0, ..., 0), where only the element tk is set to 1 and all others to 0. The goal is to learn a function to map input vector x to a probability distribution vector o = (o1, o2, ...ok, ...oK), where ok is closer to 1 and other elements are close to zero, subject to the constraint PK i=1 oi = 1. In contrast, like the perceptron approach (Crammer & Singer, 2002), our neural network approach considers the order of the categories. If a data point x belongs to category k, it is classified automatically into lowerorder categories (1, 2, ..., k − 1) as well. So the target vector of x is t = (1, 1, .., 1, 0, 0, 0), where ti (1 ≤ i ≤ k) is set to 1 and other elements zeros. Thus, the goal is to learn a function to map the input vector x to a probability vector o = (o1, o2, ..., ok, ...oK), where oi (i ≤ k) is close to 1 and oi (i ≥ k) is close to 0. PK i=1 oi is the estimate of number of categories (i.e. k) that x belongs to, instead of 1. The formulation of the target vector is similar to the perceptron approach (Crammer & Singer, 2002). It is also related to the classical cumulative probit model for ordinal regression (McCullagh, 1980), in the sense that we can consider the output probability vector (o1, ...ok, ...oK) as a cumulative probability distribution on categories (1, ..., k, ..., K), i.e., PK i=1 oi K is the proportion of categories that x belongs to, starting from category 1. The target encoding scheme of our method is related to but, different from multi-label learning (Bishop, 1996) and multiple label learning (Jin & Ghahramani, 2003) A Neural Network Approach to Ordinal Regression because our method imposes an order on the labels (or categories).\n",
      "2.2. Learning: Under the formulation, we can use the almost exactly same neural network machinery for ordinal regression. We construct a multi-layer neural network to learn ordinal relations from D. The neural network has d inputs corresponding to the number of dimensions of input feature vector x and K output nodes corresponding to K ordinal categories. There can be one or more hidden layers. Without loss of generality, we use one hidden layer to construct a standard two-layer feedforward neural network. Like a standard neural network for classification, input nodes are fully connected with hidden nodes, which in turn are fully connected with output nodes. Likewise, the transfer function of hidden nodes can be linear function, sigmoid function, and tanh function that is used in our experiment. The only difference from traditional neural network lies in the output layer. Traditional neural networks use softmax e P −zi K i=1 e−zi (or normalized exponential function) for output nodes, satisfying the constraint that the sum of outputs PK i=1 oi is 1. zi is the net input to the output node Oi . In contrast, each output node Oi of our neural network uses a standard sigmoid function 1 1+e−zi , without including the outputs from other nodes. Output node Oi is used to estimate the probability oi that a data point belongs to category i independently, without subjecting to normalization as traditional neural networks do. Thus, for a data point x of category k, the target vector is (1, , 1, .., 1, 0, 0, 0), in which the first k elements is 1 and others 0. This sets the target value of output nodes Oi (i ≤ k) to 1 and Oi (i > k) to 0. The targets instruct the neural network to adjust weights to produce probability outputs as close as possible to the target vector. It is worth pointing out that using independent sigmoid functions for output nodes does not guaranteed the monotonic relation (o1 >= o2 >= ... >= oK), which is not necessary but, desirable for making predictions (Li & Lin, 2006). A more sophisticated approach is to impose the inequality constraints on the outputs to improve the performance. Training of the neural network for ordinal regression proceeds very similarly as standard neural networks. The cost function for a data point x can be relative entropy or square error between the target vector and the output vector. For relative entropy, the cost function for output nodes is P fc = K i=1 (ti log oi + (1 − ti) log(1 − oi)). For square error, the error function is fc = PK i=1 (ti − oi) 2 . Previous studies (Richard & Lippman, 1991) on neural network cost functions show that relative entropy and square error functions usually yield very similar results. In our experiments, we use square error function and standard back-propagation to train the neural network. The errors are propagated back to output nodes, and from output nodes to hidden nodes, and finally to input nodes. Since the transfer function ft of output node Oi is the independent sigmoid function 1 1+e−zi , the derivative of ft of output node Oi is ∂ft ∂zi = e −zi (1+e−zi ) 2 = 1 1+e−zi (1 − 1 1+e−zi ) = oi(1 − oi). Thus, the net error propagated to output node Oi is ∂fc ∂oi ∂ft ∂zi = ti−oi oi(1−oi) × oi(1 − oi) = ti − oi for relative entropy cost function, ∂fc ∂oi ∂ft ∂zi = −2(ti −oi)×oi(1−oi) = −2oi(ti −oi)(1−oi) for square error cost function. The net errors are propagated through neural networks to adjust weights using gradient descent as traditional neural networks do. Despite the small difference in the transfer function and the computation of its derivative, the training of our method is the same as traditional neural networks. The network can be trained on data in the online mode where weights are updated per example, or in the batch mode where weights are updated per bunch of examples.\n",
      "2.3. Prediction: In the test phase, to make a prediction, our method scans output nodes in the order O1, O2, ..., OK. It stops when the output of a node is smaller than the predefined threshold T (e.g., 0.5) or no nodes left. The index k of the last node Ok whose output is bigger than T is the predicted category of the data point.\n",
      "3. Experiments and Results: \n",
      "3.1. Benchmark Data and Evaluation Metric: We use eight standard datasets for ordinal regression (Chu & Ghahramani, 2005a) to benchmark our method. The eight datasets (Diabetes, Pyrimidines, Triazines, Machine CUP, Auto MPG, Boston, Stocks Domain, and Abalone) are originally used for metric regression. Chu and Ghahramani (Chu & Ghahramani, 2005a) discretized the real-value targets into five equal intervals, corresponding to five ordinal categories. The authors randomly split each dataset into training/test datasets and repeated the partition 20 times independently. We use the exactly same partitions as in (Chu & Ghahramnai, 2005a) to train and test our method. A Neural Network Approach to Ordinal Regression We use the online mode to train neural networks. The parameters to tune are the number of hidden units, the number of epochs, and the learning rate. We create a grid for these three parameters, where the hidden unit number is in the range [1..15], the epoch number in the set (50, 200, 500, 1000), and the initial learning rate in the range [0.01..0.5]. During the training, the learning rate is halved if training errors continuously go up for a pre-defined number (40, 60, 80, or 100) of epochs. For experiments on each data split, the neural network parameters are fully optimized on the training data without using any test data. For each experiment, after the parameters are optimized on the training data, we train five models on the training data with the optimal parameters, starting from different initial weights. The ensemble of five trained models are then used to estimate the generalized performance on the test data. That is, the average output of five neural network models is used to make predictions. We evaluate our method using zero-one error and mean absolute error as in (Chu & Ghahramani, 2005a). Zero-one error is the percentage of wrong assignments of ordinal categories. Mean absolute error is the root mean square difference between assigned categories (k 0 ) and true categories (k) of all data points. For each dataset, the training and evaluation process is repeated 20 times on 20 data splits. Thus, we compute the average error and the standard deviation of the two metrics as in (Chu & Ghahramani, 2005a).\n",
      "3.2. Comparison with Neural Network: Classification We first compare our method (NNRank) with a standard neural network classification method (NNClass). We implement both NNRank and NNClass using C++. NNRank and NNClass share most code with minor difference in the transfer function of output nodes and its derivative computation as described in Section 2.2. As Table 1 shows, NNRank outperforms NNClass in all but one case in terms of both the mean-zero error and the mean absolute error. And on some datasets the improvement of NNRank over NNClass is sizable. For instance, on the Stock and Pyrimidines datasets, the mean zero-one error of NNRank is about 4% less than NNClass; on four datasets (Stock, Pyrimidines, Triazines, and Diabetes) the mean absolute error is reduced by about .05. The results show that the ordinal regression neural network consistently achieves the better performance than the standard classification neural network. To futher verify the effectiveness of the neural network ordinal regression approach, we are currently evaluating NNRank and NNclass on very large ordinal regression datasets in the bioinformatics domain (work in progress).\n",
      "3.3. Comparison with Gaussian Processes and: Support Vector Machines To further evaluate the performance of our method, we compare NNRank with two Gaussian process methods (GP-MAP and GP-EP) (Chu & Ghahramani, 2005a) and a support vector machine method (SVM) (Shashua & Levin, 2003) implemented in (Chu & Ghahramani, 2005a). The results of the three methods are quoted from (Chu & Ghahramani, 2005a). Table 2 reports the zero-one error on the eight datasets. NNRank achieves the best results on Diabetes, Triazines, and Abalone, GP-EP on Pyrimidines, Auto MPG, and Boston, GP-MAP on Machine, and SVM on Stocks. Table 3 reports the mean absolute error on the eight datasets. NNRank yields the best results on Diabetes and Abalone, GP-EP on Pyrimidines, Auto MPG, and Boston, GP-MAP on Triazines and Machine, SVM on Stocks. In summary, on the eight datasets, the performance of NNRank is comparable to the three state-of-the-art methods for ordinal regression.\n",
      "4. Discussion and Future Work: 4. Discussion and Future Work We have described a simple yet novel approach to adapt traditional neural networks for ordinal regression. Our neural network approach can be considered a generalization of one-layer perceptron approach (Crammer & Singer, 2002) into multi-layer. On the standard benchmark of ordinal regression, our method outperforms standard neural networks used for classification. Furthermore, on the same benchmark, our method achieves the similar performance as the two state-of-the-art methods (support vector machines and Gaussian processes) for ordinal regression. Compared with existing methods for ordinal regression, our method has several advantages of neural networks. First, like the perceptron approach (Crammer & Singer, 2002), our method can learn in both batch and online mode. The online learning ability makes our method a good tool for adaptive learning in the real-time. The multi-layer structure of neural network and the non-linear transfer function give our method the stronger fitting ability than perceptron methods. Second, the neural network can be trained on very A Neural Network Approach to Ordinal Regression large datasets iteratively, while training is more complex than support vector machines and Gaussian processes. Since the training process of our method is the same as traditional neural networks, average neural network users can use this method for their tasks. Third, neural network method can make rapid prediction once models are trained. The ability of learning on very large dataset and predicting in time makes our method a useful and competitive tool for ordinal regression tasks, particularly for time-critical and large-scale ranking problems in information retrieval, web page ranking, collaborative filtering, and the emerging fields of Bioinformatics. We are currently applying the method to rank proteins according to their structural relevance with respect to a query protein (Cheng & Baldi, 2006). To facilitate the application of this new approach, we make both NNRank and NNClass to accept a general input format and freely available at http://www.eecs.ucf.edu/∼jcheng/cheng software.html. There are some directions to further improve the neural network (or multi-layer perceptron) approach for ordinal regression. One direction is to design a transfer function to ensure the monotonic decrease of the outputs of the neural network; the other direction is to derive the general error bounds of the method under the binary classification framework (Li & Lin, 2006). Furthermore, the other flavors of implementations of the multi-threshold multi-layer perceptron approach for ordinal regression are possible. Since machine learning ranking is a fundamental problem that has wide applications in many diverse domains such as web page ranking, information retrieval, image retrieval, collaborative filtering, bioinformatics and so on, we believe the further exploration of the neural network (or multi-layer perceptron) approach for ranking and ordinal regression is worthwhile. References Agarwal, S., & Roth, D. (2005). Learnability of bipartite ranking functions. In Proc. of the 18th annual conference on learning theory (colt-05). Aiolli, F., & Sperduti, A. (2004). Learning preferences for multiclass problems. In Advances in neural information processing systems 17 (nips). Basilico, J., & Hofmann, T. (2004). Unifying collaborative and content-based filtering. In Proceedings of the twenty-first international conference on machine learning (icml), 9. New York, USA: ACM press. Bishop, C. (1996). Neural networks for pattern recognition. USA: Oxford University Press. Burges, C., Ragno, R., & Le, Q. V. (2006). Learning to rank with nonsmooth cost functions. In Advances in neural information processing systems (nips) 20. Cambridge, MA: MIT press. Burges, C. J. C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., & Hullender, G. (2005). Learning to rank using gradient descent. In Proc. of internaltional conference on machine learning (icml05), 89–97. Caruana, R., Baluja, S., & Mitchell, T. (1996). Using the future to sort out the present: Rankprop and multitask learning for medical risk evaluation. In Advances in neural information processing systems 8 (nips). Cheng, J., & Baldi, P. (2006). A machine learning information retrieval approach to protein fold recognition. Bioinformatics, 22, 1456–1463. Chu, W., & Ghahramani, Z. (2005a). Gaussian processes for ordinal regression. Journal of Machine Learning Research, 6, 1019–1041. Chu, W., & Ghahramani, Z. (2005b). Preference learning with Gaussian processes. In Proc. of international conference on machine learning (icml-05), 137–144. Chu, W., & Keerthi, S. (2005). New approaches to support vector ordinal regression. In Proc. of international conference on machine learning (icml-05), 145–152. Chu, W., & Keerthi, S. (2007). Support vector ordinal regression. Neural Computation, 19. Cohen, W. W., Schapire, R. E., & Singer, Y. (1999). Learning to order things. Journal of Artificial Intelligence Research, 10, 243–270. Crammer, K., & Singer, Y. (2002). Pranking with ranking. In Advances in neural information processing systems (nips) 14, 641–647. Cambridge, MA: MIT press. Dekel, O., Keshet, J., & Singer, Y. (2004). Log-linear models for label ranking. In Proc. of the 21st international conference on machine learning (icml-06), 209–216. Frank, E., & Hall, M. (2001). A simple approach to ordinal classification. In Proc. of the european conference on machine learning. A Neural Network Approach to Ordinal Regression Freund, Y., Iyer, R., Schapire, R., & Singer, Y. (2003). An efficient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4, 933–969. Goldberg, D., Nichols, D., Oki, B., & Terry, D. (1992). Using collaborative filtering to weave an information tapestry. Communications of the ACM, 35, 61–70. Har-Peled, S., Roth, D., & Zimak, D. (2002). Constraint classification: a new approach to multiclass classification and ranking. In Advances in neural information processing systems 15 (nips). Herbrich, R., Graepel, T., Bollmann-Sdorra, P., & Obermayer, K. (1998). Learning preference relations for information retrieval. In Proc. of icml workshop on text categorization and machine learning, 80–84. Herbrich, R., Graepel, T., & Obermayer, K. (1999). Support vector learning for ordinal regression. In Proc. of 9th international conference on artificial neural networks (icann), 97–102. Herbrich, R., Graepel, T., & Obermayer, K. (2000). Large margin rank boundaries for ordinal regression. In A. J. Smola, P. Bartlett, B. Scholkopf and D. Schuurmans (Eds.), Advances in large margin classifiers, 115–132. Cambridge, MA: MIT Press. Jin, R., & Ghahramani, Z. (2003). Learning with multiple labels. In Advances in neural information processing systems (nips) 15. Cambridge, MA: MIT press. Joachims, I. (2002). Optimizing search engines using clickthrough data. In D. Hand, D. Keim and R. NG (Eds.), Proc. of 8th acm sigkdd international conference on knowledge discovery and data mining, 133–142. Kramer, S., Widmer, G., Pfahringer, B., & DeGroeve, M. (2001). Prediction of ordinal classes using regression trees. Fundamenta Informaticae, 47, 1–13. Li, L., & Lin, H. (2006). Ordinal regression by extended binary classification. In Advances in neural information processing systems (nips) 20. Cambridge, MA: MIT press. MacKay, D. J. C. (1992). A practical bayesian framework for back propagation networks. Neural Computation, 4, 448–472. McCullagh, P. (1980). Regression models for ordinal data. Journal of the Royal Statistical Society B, 42, 109–142. McCullagh, P., & Nelder, J. A. (1983). Generalized linear models. London: Chapman and Hall. Minka, T. P. (2001). A family of algorithms for approximate bayesian inference. PhD Thesis, Massachusetts Institute of Technology. Paquet, U., Holden, S., & Naish-Guzman, A. (2005). Bayesian hierarchical ordinal regression. In Proc. of the international conference on artifical neural networks. Rajaram, S., Garg, A., Zhou, X., & Huang, T. (2003). Classification approach towards ranking and sorting problems. In Machine learning: Ecml 2003, vol. 2837 of lecture notes in artificail intelligence (n. lavrac, d. gamberger, h. blockeel and l. todorovski eds.), 301–312. Springer-Verlag. Richard, M., & Lippman, R. (1991). Neural network classifiers estimate bayesian a-posteriori probabilities. Neural Computation, 3, 461–483. Rumelhart, D., Hinton, G., & Williams, R. (1986). Learning Internal Representations by Error Propagation. In D. E. Rumelhart and J. L. McClelland (Eds.), Parallel distributed processing: Explorations in the microstructure of cognition. vol. i: Foundations, 318–362. Bradford Books/MIT Press, Cambridge, MA. Sch¨olkopf, B., & Smola, A. (2002). Learning with Kernels, Support Vector Machines, Regularization, Optimization and Beyond. Cambridge, MA: MIT University Press. Schwaighofer, A., Tresp, V., & Yu, K. (2005). Hiearachical bayesian modelling with gaussian processes. In Advances in neural information processing systems 17 (nips). MIT press. Shashua, A., & Levin, A. (2003). Ranking with large margin principle: two approaches. In Advances in neural information processing systems 15 (nips). Vapnik, V. (1995). The nature of statistical learning theory. Berlin, Germany: Springer-Verlag. Wu, H., Lu, H., & Ma, S. (2003). A practical svmbased algorithm for ordinal regression in image retrieval. 612–621. Yu, S., Yu, K., Tresp, V., & Kriegel, H. P. (2006). Collaborative ordinal regression. In Proc. of 23rd international conference on machine learning, 1089– 1096. A Neural Network Approach to Ordinal Regression Zhang, H., Jiang, L., & Su, J. (2005). Augmenting naive bayes for ranking. In International conference on machine learning (icml-05). A Neural Network Approach to Ordinal Regression Table 1. The results of NNRank and NNClass on the eight datasets. The results are the average error over 20 trials along with the standard deviation. Mean zero-one error Mean absolute error Dataset NNRank NNClass NNRank NNClass Stocks 12.68±1.8% 16.97± 2.3% 0.127±0.01 0.173±0.02 Pyrimidines 37.71±8.1% 41.87±7.9% 0.450±0.09 0.508±0.11 Auto MPG 27.13±2.0% 28.82±2.7% 0.281±0.02 0.307±0.03 Machine 17.03±4.2% 17.80±4.4% 0.186±0.04 0.192±0.06 Abalone 21.39±0.3% 21.74± 0.4% 0.226±0.01 0.232±0.01 Triazines 52.55±5.0% 52.84±5.9% 0.730±0.06 0.790±0.09 Boston 26.38±3.0% 26.62±2.7% 0.295±0.03 0.297±0.03 Diabetes 44.90±12.5% 43.84±10.0% 0.546±0.15 0.592±0.09 Table 2. Zero-one error of NNRank, SVM, GP-MAP, and GP-EP on the eight datasets. SVM denotes the support vector machine method (Shashua & Levin, 2003; Chu & Ghahramani, 2005a). GP-MAP and GP-EP are two Gaussian process methods using Laplace approximation (MacKay, 1992) and expectation propagation (Minka, 2001) respectively (Chu & Ghahramani, 2005a). The results are the average error over 20 trials along with the standard deviation. We use boldface to denote the best results. Data NNRank SVM GP-MAP GP-EP Triazines 52.55±5.0% 54.19±1.5% 52.91±2.2% 52.62±2.7% Pyrimidines 37.71±8.1% 41.46±8.5% 39.79±7.2% 36.46±6.5% Diabetes 44.90±12.5% 57.31±12.1% 54.23±13.8% 54.23±13.8% Machine 17.03±4.2% 17.37±3.6% 16.53±3.6% 16.78±3.9% Auto MPG 27.13±2.0% 25.73±2.2% 23.78±1.9% 23.75±1.7% Boston 26.38±3.0% 25.56±2.0% 24.88±2.0% 24.49±1.9% Stocks 12.68±1.8% 10.81±1.7% 11.99±2.3% 12.00±2.1% Abalone 21.39±0.3% 21.58±0.3% 21.50±0.2% 21.56±0.4% Table 3. Mean absolute error of NNRank, SVM, GP-MAP, and GP-EP on the eight datasets. SVM denotes the support vector machine method (Shashua & Levin, 2003; Chu & Ghahramani, 2005a). GP-MAP and GP-EP are two Gaussian process methods using Laplace approximation and expectation propagation respectively (Chu & Ghahramani, 2005a). The results are the average error over 20 trials along with the standard deviation. We use boldface to denote the best results. Data NNRank SVM GP-MAP GP-EP Triazines 0.730±0.07 0.698±0.03 0.687±0.02 0.688±0.03 Pyrimidines 0.450±0.10 0.450±0.11 0.427±0.09 0.392±0.07 Diabetes 0.546±0.15 0.746±0.14 0.662±0.14 0.665±0.14 Machine 0.186±0.04 0.192±0.04 0.185±0.04 0.186±0.04 Auto MPG 0.281±0.02 0.260±0.02 0.241±0.02 0.241±0.02 Boston 0.295±0.04 0.267±0.02 0.260±0.02 0.259±0.02 Stocks 0.127±0.02 0.108±0.02 0.120±0.02 0.120±0.02 Abalone 0.226±0.01 0.229±0.01 0.232±0.01 0.234±0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_section_texts = ''\n",
    "for k in section_texts:\n",
    "    full_section_texts += k\n",
    "    full_section_texts += ': '\n",
    "    full_section_texts += section_texts[k]\n",
    "    full_section_texts += '\\n'\n",
    "print(full_section_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruoqwang/Library/Python/3.12/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "#load Pegasus model & tokenizer\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ruoqwang/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_tokens=512):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "        sentence_length = len(tokenized_sentence)\n",
    "        length = current_length + sentence_length\n",
    "        #if adding the sentence exceeds max_tokens, start a new chunk\n",
    "        if current_length + sentence_length > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    #append the last chunk if not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunk(text, max_length=50):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    print('try to generate with model')\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,  \n",
    "        min_length=10,  \n",
    "        num_beams=8,  \n",
    "        length_penalty=0.5,  \n",
    "        early_stopping=True\n",
    "    )\n",
    "    res = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recursively summarize full text of each section\n",
    "def summarize_full_text(text):\n",
    "    chunks = chunk_text(text)\n",
    "    summaries = [summarize_chunk(chunk) for chunk in chunks]\n",
    "    combined_summary = \" \".join(summaries)\n",
    "    final_summary = summarize_chunk(combined_summary, max_length=30)\n",
    "\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "We present a new method for the classification of large-scale unstructured data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summary = summarize_full_text(section_texts['1. Introduction'])\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(41, '1. Introduction'), (165, '2. Method'), (166, '2.1. Formulation'), (217, '2.2. Learning'), (321, '2.3. Prediction'), (329, '3. Experiments and Results'), (330, '3.1. Benchmark Data and Evaluation Metric'), (378, '3.2. Comparison with Neural Network'), (403, '3.3. Comparison with Gaussian Processes and'), (425, '4. Discussion and Future Work')]\n"
     ]
    }
   ],
   "source": [
    "print(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1. Introduction': 'Ordinal regression (or ranking learning) is an impor- tant supervised problem of learning a ranking or or- dering on instances, which has the property of both classiﬁcation and metric regression. The learning task of ordinal regression is to assign data points into a set of ﬁnite ordered categories. For example, a teacher rates students’ performance using A, B, C, D, and E (A > B > C > D > E) (Chu & Ghahramani, 2005a). Ordinal regression is diﬀerent from classiﬁcation due to the order of categories. In contrast to metric re- gression, the response variables (categories) in ordinal regression is discrete and ﬁnite. The research of ordinal regression dated back to the ordinal statistics methods in 1980s (McCullagh, 1980; McCullagh & Nelder, 1983) and machine learning re- search in 1990s (Caruana et al., 1996; Herbrich et al., 1998; Cohen et al., 1999). It has attracted the con- siderable attention in recent years due to its poten- tial applications in many data-intensive domains such as information retrieval (Herbrich et al., 1998), web page ranking (Joachims, 2002), collaborative ﬁltering (Goldberg et al., 1992; Basilico & Hofmann, 2004; Yu et al., 2006), image retrieval (Wu et al., 2003), and pro- tein ranking (Cheng & Baldi, 2006) in Bioinformatics. A number of machine learning methods have been de- veloped or redesigned to address ordinal regression problem (Rajaram et al., 2003), including perceptron (Crammer & Singer, 2002) and its kernelized gener- alization (Basilico & Hofmann, 2004), neural network with gradient descent (Caruana et al., 1996; Burges et al., 2005), Gaussian process (Chu & Ghahramani, 2005b; Chu & Ghahramani, 2005a; Schwaighofer et al., 2005), large margin classiﬁer (or support vec- tor machine) (Herbrich et al., 1999; Herbrich et al., 2000; Joachims, 2002; Shashua & Levin, 2003; Chu & Keerthi, 2005; Aiolli & Sperduti, 2004; Chu & Keerthi, 2007), k-partite classiﬁer (Agarwal & Roth, 2005), boosting algorithm (Freund et al., 2003; Dekel et al., 2002), constraint classiﬁcation (Har-Peled et al., 2002), regression trees (Kramer et al., 2001), Naive Bayes (Zhang et al., 2005), Bayesian hierarchical ex- perts (Paquet et al., 2005), binary classiﬁcation ap- proach (Frank & Hall, 2001; Li & Lin, 2006) that de- composes the original ordinal regression problem into a set of binary classiﬁcations, and the optimization of nonsmooth cost functions (Burges et al., 2006). Most of these methods can be roughly classiﬁed into two categories: pairwise constraint approach (Herbrich et al., 2000; Joachims, 2002; Dekel et al., 2004; Burges et al., 2005) and multi-threshold approach (Cram- mer & Singer, 2002; Shashua & Levin, 2003; Chu & Ghahramani, 2005a). The former is to convert the full ranking relation into pairwise order constraints. The latter tries to learn multiple thresholds to divide data arXiv:0704.1028v1  [cs.LG]  8 Apr 2007  A Neural Network Approach to Ordinal Regression into ordinal categories. Multi-threshold approaches also can be uniﬁed under the general, extended binary classiﬁcation framework (Li & Lin, 2006). The ordinal regression methods have diﬀerent advan- tages and disadvantages. Prank (Crammer & Singer, 2002), a perceptron approach that generalizes the bi- nary perceptron algorithm to the ordinal multi-class situation, is a fast online algorithm. However, like a standard perceptron method, its accuracy suﬀers when dealing with non-linear data, while a quadratic kernel version of Prank greatly relieves this problem. One class of accurate large-margin classiﬁer approaches (Herbrich et al., 2000; Joachims, 2002) convert the ordinal relations into O(n2) (n: the number of data points) pairwise ranking constraints for the structural risk minimization (Vapnik, 1995; Schoelkopf & Smola, 2002). Thus, it can not be applied to medium size datasets (> 10,000 data points), without discarding some pairwise preference relations. It may also overﬁt noise due to incomparable pairs. The other class of powerful large-margin classiﬁer methods (Shashua & Levin, 2003; Chu & Keerthi, 2005) generalize the support vector formulation for or- dinal regression by ﬁnding K −1 thresholds on the real line that divide data into K ordered categories. The size of this optimization problem is linear in the number of training examples. However, like support vector machine used for classiﬁcation, the prediction speed is slow when the solution is not sparse, which makes it not appropriate for time-critical tasks. Simi- larly, another state-of-the-art approach, Gaussian pro- cess method (Chu & Ghahramani, 2005a), also has the diﬃculty of handling large training datasets and the problem of slow prediction speed in some situations. Here we describe a new neural network approach for ordinal regression that has the advantages of neural network learning: learning in both online and batch mode, training on very large dataset (Burges et al., 2005), handling non-linear data, good performance, and rapid prediction. Our method can be considered a generalization of the perceptron learning (Crammer & Singer, 2002) into multi-layer perceptrons (neural network) for ordinal regression. Our method is also related to the classic generalized linear models (e.g., cumulative logit model) for ordinal regression (Mc- Cullagh, 1980). Unlike the neural network method (Burges et al., 2005) trained on pairs of examples to learn pairwise order relations, our method works on individual data points and uses multiple output nodes to estimate the probabilities of ordinal cate- gories. Thus, our method falls into the category of multi-threshold approach. The learning of our method proceeds similarly as traditional neural networks using back-propagation (Rumelhart et al., 1986). On the same benchmark datasets, our method yields the performance better than the standard classiﬁca- tion neural networks and comparable to the state-of- the-art methods using support vector machines and Gaussian processes. In addition, our method can learn on very large datasets and make rapid predictions.', '2. Method': '', '2.1. Formulation': 'Let D represent an ordinal regression dataset consist- ing of n data points (x, y) , where x ∈Rd is an input feature vector and y is its ordinal category from a ﬁ- nite set Y . Without loss of generality, we assume that Y = 1, 2, ..., K with ”<” as order relation. For a standard classiﬁcation neural network without considering the order of categories, the goal is to pre- dict the probability of a data point x belonging to one category k (y = k). The input is x and the target of encoding the category k is a vector t = (0, ..., 0, 1, 0, ..., 0), where only the element tk is set to 1 and all others to 0. The goal is to learn a function to map input vector x to a probability distribution vector o = (o1, o2, ...ok, ...oK), where ok is closer to 1 and other elements are close to zero, subject to the constraint PK i=1 oi = 1. In contrast, like the perceptron approach (Crammer & Singer, 2002), our neural network approach considers the order of the categories. If a data point x belongs to category k, it is classiﬁed automatically into lower- order categories (1, 2, ..., k −1) as well. So the target vector of x is t = (1, 1, .., 1, 0, 0, 0), where ti (1 ≤i ≤k) is set to 1 and other elements zeros. Thus, the goal is to learn a function to map the input vector x to a probability vector o = (o1, o2, ..., ok, ...oK), where oi (i ≤k) is close to 1 and oi (i ≥k) is close to 0. PK i=1 oi is the estimate of number of categories (i.e. k) that x belongs to, instead of 1. The formulation of the target vector is similar to the perceptron ap- proach (Crammer & Singer, 2002). It is also related to the classical cumulative probit model for ordinal re- gression (McCullagh, 1980), in the sense that we can consider the output probability vector (o1, ...ok, ...oK) as a cumulative probability distribution on categories (1, ..., k, ..., K), i.e., PK i=1 oi K is the proportion of cate- gories that x belongs to, starting from category 1. The target encoding scheme of our method is related to but, diﬀerent from multi-label learning (Bishop, 1996) and multiple label learning (Jin & Ghahramani, 2003)  A Neural Network Approach to Ordinal Regression because our method imposes an order on the labels (or categories).', '2.2. Learning': 'Under the formulation, we can use the almost exactly same neural network machinery for ordinal regression. We construct a multi-layer neural network to learn ordinal relations from D. The neural network has d inputs corresponding to the number of dimensions of input feature vector x and K output nodes correspond- ing to K ordinal categories. There can be one or more hidden layers. Without loss of generality, we use one hidden layer to construct a standard two-layer feedfor- ward neural network. Like a standard neural network for classiﬁcation, input nodes are fully connected with hidden nodes, which in turn are fully connected with output nodes. Likewise, the transfer function of hid- den nodes can be linear function, sigmoid function, and tanh function that is used in our experiment. The only diﬀerence from traditional neural network lies in the output layer. Traditional neural networks use soft- max e−zi PK i=1 e−zi (or normalized exponential function) for output nodes, satisfying the constraint that the sum of outputs PK i=1 oi is 1. zi is the net input to the output node Oi. In contrast, each output node Oi of our neural net- work uses a standard sigmoid function 1 1+e−zi , with- out including the outputs from other nodes. Output node Oi is used to estimate the probability oi that a data point belongs to category i independently, with- out subjecting to normalization as traditional neural networks do. Thus, for a data point x of category k, the target vector is (1, , 1, .., 1, 0, 0, 0), in which the ﬁrst k elements is 1 and others 0. This sets the target value of output nodes Oi (i ≤k) to 1 and Oi (i > k) to 0. The targets instruct the neural network to ad- just weights to produce probability outputs as close as possible to the target vector. It is worth pointing out that using independent sigmoid functions for out- put nodes does not guaranteed the monotonic relation (o1 >= o2 >= ... >= oK), which is not necessary but, desirable for making predictions (Li & Lin, 2006). A more sophisticated approach is to impose the inequal- ity constraints on the outputs to improve the perfor- mance. Training of the neural network for ordinal regres- sion proceeds very similarly as standard neural net- works. The cost function for a data point x can be relative entropy or square error between the tar- get vector and the output vector. For relative en- tropy, the cost function for output nodes is fc = PK i=1 (ti log oi + (1 −ti) log(1 −oi)). For square er- ror, the error function is fc = PK i=1 (ti −oi)2. Pre- vious studies (Richard & Lippman, 1991) on neural network cost functions show that relative entropy and square error functions usually yield very similar re- sults. In our experiments, we use square error function and standard back-propagation to train the neural net- work. The errors are propagated back to output nodes, and from output nodes to hidden nodes, and ﬁnally to input nodes. Since the transfer function ft of output node Oi is the independent sigmoid function 1 1+e−zi , the deriva- tive of ft of output node Oi is ∂ft ∂zi = e−zi (1+e−zi)2 = 1 1+e−zi (1 − 1 1+e−zi ) = oi(1 −oi). Thus, the net error propagated to output node Oi is ∂fc ∂oi ∂ft ∂zi = ti−oi oi(1−oi) × oi(1 −oi) = ti −oi for relative entropy cost function, ∂fc ∂oi ∂ft ∂zi = −2(ti −oi)×oi(1−oi) = −2oi(ti −oi)(1−oi) for square error cost function. The net errors are prop- agated through neural networks to adjust weights us- ing gradient descent as traditional neural networks do. Despite the small diﬀerence in the transfer function and the computation of its derivative, the training of our method is the same as traditional neural networks. The network can be trained on data in the online mode where weights are updated per example, or in the batch mode where weights are updated per bunch of examples.', '2.3. Prediction': 'In the test phase, to make a prediction, our method scans output nodes in the order O1, O2, ..., OK. It stops when the output of a node is smaller than the predeﬁned threshold T (e.g., 0.5) or no nodes left. The index k of the last node Ok whose output is bigger than T is the predicted category of the data point.', '3. Experiments and Results': '', '3.1. Benchmark Data and Evaluation Metric': 'We use eight standard datasets for ordinal regres- sion (Chu & Ghahramani, 2005a) to benchmark our method. The eight datasets (Diabetes, Pyrimidines, Triazines, Machine CUP, Auto MPG, Boston, Stocks Domain, and Abalone) are originally used for metric regression. Chu and Ghahramani (Chu & Ghahra- mani, 2005a) discretized the real-value targets into ﬁve equal intervals, corresponding to ﬁve ordinal cat- egories. The authors randomly split each dataset into training/test datasets and repeated the partition 20 times independently. We use the exactly same parti- tions as in (Chu & Ghahramnai, 2005a) to train and test our method.  A Neural Network Approach to Ordinal Regression We use the online mode to train neural networks. The parameters to tune are the number of hidden units, the number of epochs, and the learning rate. We create a grid for these three parameters, where the hidden unit number is in the range [1..15], the epoch number in the set (50, 200, 500, 1000), and the initial learning rate in the range [0.01..0.5]. During the training, the learning rate is halved if training errors continuously go up for a pre-deﬁned number (40, 60, 80, or 100) of epochs. For experiments on each data split, the neural network parameters are fully optimized on the training data without using any test data. For each experiment, after the parameters are opti- mized on the training data, we train ﬁve models on the training data with the optimal parameters, start- ing from diﬀerent initial weights. The ensemble of ﬁve trained models are then used to estimate the general- ized performance on the test data. That is, the average output of ﬁve neural network models is used to make predictions. We evaluate our method using zero-one error and mean absolute error as in (Chu & Ghahramani, 2005a). Zero-one error is the percentage of wrong assignments of ordinal categories. Mean absolute error is the root mean square diﬀerence between assigned categories (k′) and true categories (k) of all data points. For each dataset, the training and evaluation process is repeated 20 times on 20 data splits. Thus, we com- pute the average error and the standard deviation of the two metrics as in (Chu & Ghahramani, 2005a).', '3.2. Comparison with Neural Network': 'Classiﬁcation We ﬁrst compare our method (NNRank) with a stan- dard neural network classiﬁcation method (NNClass). We implement both NNRank and NNClass using C++. NNRank and NNClass share most code with minor diﬀerence in the transfer function of output nodes and its derivative computation as described in Section 2.2. As Table 1 shows, NNRank outperforms NNClass in all but one case in terms of both the mean-zero error and the mean absolute error. And on some datasets the improvement of NNRank over NNClass is sizable. For instance, on the Stock and Pyrimidines datasets, the mean zero-one error of NNRank is about 4% less than NNClass; on four datasets (Stock, Pyrimidines, Triazines, and Diabetes) the mean absolute error is reduced by about .05. The results show that the or- dinal regression neural network consistently achieves the better performance than the standard classiﬁca- tion neural network. To futher verify the eﬀectiveness of the neural network ordinal regression approach, we are currently evaluating NNRank and NNclass on very large ordinal regression datasets in the bioinformatics domain (work in progress).', '3.3. Comparison with Gaussian Processes and': 'Support Vector Machines To further evaluate the performance of our method, we compare NNRank with two Gaussian process meth- ods (GP-MAP and GP-EP) (Chu & Ghahramani, 2005a) and a support vector machine method (SVM) (Shashua & Levin, 2003) implemented in (Chu & Ghahramani, 2005a). The results of the three meth- ods are quoted from (Chu & Ghahramani, 2005a). Ta- ble 2 reports the zero-one error on the eight datasets. NNRank achieves the best results on Diabetes, Tri- azines, and Abalone, GP-EP on Pyrimidines, Auto MPG, and Boston, GP-MAP on Machine, and SVM on Stocks. Table 3 reports the mean absolute error on the eight datasets. NNRank yields the best results on Diabetes and Abalone, GP-EP on Pyrimidines, Auto MPG, and Boston, GP-MAP on Triazines and Machine, SVM on Stocks. In summary, on the eight datasets, the performance of NNRank is comparable to the three state-of-the-art methods for ordinal regression.', '4. Discussion and Future Work': '4. Discussion and Future Work We have described a simple yet novel approach to adapt traditional neural networks for ordinal regres- sion. Our neural network approach can be consid- ered a generalization of one-layer perceptron approach (Crammer & Singer, 2002) into multi-layer. On the standard benchmark of ordinal regression, our method outperforms standard neural networks used for classi- ﬁcation. Furthermore, on the same benchmark, our method achieves the similar performance as the two state-of-the-art methods (support vector machines and Gaussian processes) for ordinal regression. Compared with existing methods for ordinal regres- sion, our method has several advantages of neural net- works. First, like the perceptron approach (Crammer & Singer, 2002), our method can learn in both batch and online mode. The online learning ability makes our method a good tool for adaptive learning in the real-time. The multi-layer structure of neural network and the non-linear transfer function give our method the stronger ﬁtting ability than perceptron methods. Second, the neural network can be trained on very  A Neural Network Approach to Ordinal Regression large datasets iteratively, while training is more com- plex than support vector machines and Gaussian pro- cesses. Since the training process of our method is the same as traditional neural networks, average neural network users can use this method for their tasks. Third, neural network method can make rapid prediction once models are trained. The ability of learning on very large dataset and predicting in time makes our method a useful and competitive tool for ordinal regression tasks, particularly for time-critical and large-scale ranking problems in information retrieval, web page ranking, collaborative ﬁltering, and the emerging ﬁelds of Bioinformat- ics. We are currently applying the method to rank proteins according to their structural rele- vance with respect to a query protein (Cheng & Baldi, 2006). To facilitate the application of this new approach, we make both NNRank and NNClass to accept a general input format and freely available at http://www.eecs.ucf.edu/∼jcheng/cheng software.html. There are some directions to further improve the neu- ral network (or multi-layer perceptron) approach for ordinal regression. One direction is to design a trans- fer function to ensure the monotonic decrease of the outputs of the neural network; the other direction is to derive the general error bounds of the method under the binary classiﬁcation framework (Li & Lin, 2006). Furthermore, the other ﬂavors of implemen- tations of the multi-threshold multi-layer perceptron approach for ordinal regression are possible. Since ma- chine learning ranking is a fundamental problem that has wide applications in many diverse domains such as web page ranking, information retrieval, image re- trieval, collaborative ﬁltering, bioinformatics and so on, we believe the further exploration of the neural net- work (or multi-layer perceptron) approach for ranking and ordinal regression is worthwhile. References Agarwal, S., & Roth, D. (2005). Learnability of bipar- tite ranking functions. In Proc. of the 18th annual conference on learning theory (colt-05). Aiolli, F., & Sperduti, A. (2004). Learning preferences for multiclass problems. In Advances in neural in- formation processing systems 17 (nips). Basilico, J., & Hofmann, T. (2004). Unifying collabo- rative and content-based ﬁltering. In Proceedings of the twenty-ﬁrst international conference on machine learning (icml), 9. New York, USA: ACM press. Bishop, C. (1996). Neural networks for pattern recog- nition. USA: Oxford University Press. Burges, C., Ragno, R., & Le, Q. V. (2006). Learning to rank with nonsmooth cost functions. In Advances in neural information processing systems (nips) 20. Cambridge, MA: MIT press. Burges, C. J. C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., & Hullender, G. (2005). Learning to rank using gradient descent. In Proc. of internaltional conference on machine learning (icml- 05), 89–97. Caruana, R., Baluja, S., & Mitchell, T. (1996). Using the future to sort out the present: Rankprop and multitask learning for medical risk evaluation. In Advances in neural information processing systems 8 (nips). Cheng, J., & Baldi, P. (2006). A machine learning in- formation retrieval approach to protein fold recog- nition. Bioinformatics, 22, 1456–1463. Chu, W., & Ghahramani, Z. (2005a). Gaussian pro- cesses for ordinal regression. Journal of Machine Learning Research, 6, 1019–1041. Chu, W., & Ghahramani, Z. (2005b). Preference learn- ing with Gaussian processes. In Proc. of inter- national conference on machine learning (icml-05), 137–144. Chu, W., & Keerthi, S. (2005). New approaches to support vector ordinal regression. In Proc. of inter- national conference on machine learning (icml-05), 145–152. Chu, W., & Keerthi, S. (2007). Support vector ordinal regression. Neural Computation, 19. Cohen, W. W., Schapire, R. E., & Singer, Y. (1999). Learning to order things. Journal of Artiﬁcial Intel- ligence Research, 10, 243–270. Crammer, K., & Singer, Y. (2002). Pranking with ranking. In Advances in neural information pro- cessing systems (nips) 14, 641–647. Cambridge, MA: MIT press. Dekel, O., Keshet, J., & Singer, Y. (2004). Log-linear models for label ranking. In Proc. of the 21st inter- national conference on machine learning (icml-06), 209–216. Frank, E., & Hall, M. (2001). A simple approach to ordinal classiﬁcation. In Proc. of the european con- ference on machine learning.  A Neural Network Approach to Ordinal Regression Freund, Y., Iyer, R., Schapire, R., & Singer, Y. (2003). An eﬃcient boosting algorithm for combining pref- erences. Journal of Machine Learning Research, 4, 933–969. Goldberg, D., Nichols, D., Oki, B., & Terry, D. (1992). Using collaborative ﬁltering to weave an information tapestry. Communications of the ACM, 35, 61–70. Har-Peled, S., Roth, D., & Zimak, D. (2002). Con- straint classiﬁcation: a new approach to multiclass classiﬁcation and ranking. In Advances in neural information processing systems 15 (nips). Herbrich, R., Graepel, T., Bollmann-Sdorra, P., & Obermayer, K. (1998). Learning preference relations for information retrieval. In Proc. of icml workshop on text categorization and machine learning, 80–84. Herbrich, R., Graepel, T., & Obermayer, K. (1999). Support vector learning for ordinal regression. In Proc. of 9th international conference on artiﬁcial neural networks (icann), 97–102. Herbrich, R., Graepel, T., & Obermayer, K. (2000). Large margin rank boundaries for ordinal regres- sion. In A. J. Smola, P. Bartlett, B. Scholkopf and D. Schuurmans (Eds.), Advances in large margin classiﬁers, 115–132. Cambridge, MA: MIT Press. Jin, R., & Ghahramani, Z. (2003). Learning with multiple labels. In Advances in neural information processing systems (nips) 15. Cambridge, MA: MIT press. Joachims, I. (2002). Optimizing search engines us- ing clickthrough data. In D. Hand, D. Keim and R. NG (Eds.), Proc. of 8th acm sigkdd international conference on knowledge discovery and data mining, 133–142. Kramer, S., Widmer, G., Pfahringer, B., & DeGroeve, M. (2001). Prediction of ordinal classes using regres- sion trees. Fundamenta Informaticae, 47, 1–13. Li, L., & Lin, H. (2006). Ordinal regression by ex- tended binary classiﬁcation. In Advances in neu- ral information processing systems (nips) 20. Cam- bridge, MA: MIT press. MacKay, D. J. C. (1992). A practical bayesian frame- work for back propagation networks. Neural Com- putation, 4, 448–472. McCullagh, P. (1980). Regression models for ordinal data. Journal of the Royal Statistical Society B, 42, 109–142. McCullagh, P., & Nelder, J. A. (1983). Generalized linear models. London: Chapman and Hall. Minka, T. P. (2001). A family of algorithms for ap- proximate bayesian inference. PhD Thesis, Mas- sachusetts Institute of Technology. Paquet, U., Holden, S., & Naish-Guzman, A. (2005). Bayesian hierarchical ordinal regression. In Proc. of the international conference on artiﬁcal neural net- works. Rajaram, S., Garg, A., Zhou, X., & Huang, T. (2003). Classiﬁcation approach towards ranking and sort- ing problems. In Machine learning: Ecml 2003, vol. 2837 of lecture notes in artiﬁcail intelligence (n. lavrac, d. gamberger, h. blockeel and l. todorovski eds.), 301–312. Springer-Verlag. Richard, M., & Lippman, R. (1991). Neural network classiﬁers estimate bayesian a-posteriori probabili- ties. Neural Computation, 3, 461–483. Rumelhart, D., Hinton, G., & Williams, R. (1986). Learning Internal Representations by Error Propa- gation. In D. E. Rumelhart and J. L. McClelland (Eds.), Parallel distributed processing: Explorations in the microstructure of cognition. vol. i: Founda- tions, 318–362. Bradford Books/MIT Press, Cam- bridge, MA. Sch¨olkopf, B., & Smola, A. (2002). Learning with Ker- nels, Support Vector Machines, Regularization, Op- timization and Beyond. Cambridge, MA: MIT Uni- versity Press. Schwaighofer, A., Tresp, V., & Yu, K. (2005). Hiear- achical bayesian modelling with gaussian processes. In Advances in neural information processing sys- tems 17 (nips). MIT press. Shashua, A., & Levin, A. (2003). Ranking with large margin principle: two approaches. In Advances in neural information processing systems 15 (nips). Vapnik, V. (1995). The nature of statistical learning theory. Berlin, Germany: Springer-Verlag. Wu, H., Lu, H., & Ma, S. (2003). A practical svm- based algorithm for ordinal regression in image re- trieval. 612–621. Yu, S., Yu, K., Tresp, V., & Kriegel, H. P. (2006). Collaborative ordinal regression. In Proc. of 23rd international conference on machine learning, 1089– 1096.  A Neural Network Approach to Ordinal Regression Zhang, H., Jiang, L., & Su, J. (2005). Augmenting naive bayes for ranking. In International conference on machine learning (icml-05).  A Neural Network Approach to Ordinal Regression Table 1. The results of NNRank and NNClass on the eight datasets. The results are the average error over 20 trials along with the standard deviation. Mean zero-one error Mean absolute error Dataset NNRank NNClass NNRank NNClass Stocks 12.68±1.8% 16.97± 2.3% 0.127±0.01 0.173±0.02 Pyrimidines 37.71±8.1% 41.87±7.9% 0.450±0.09 0.508±0.11 Auto MPG 27.13±2.0% 28.82±2.7% 0.281±0.02 0.307±0.03 Machine 17.03±4.2% 17.80±4.4% 0.186±0.04 0.192±0.06 Abalone 21.39±0.3% 21.74± 0.4% 0.226±0.01 0.232±0.01 Triazines 52.55±5.0% 52.84±5.9% 0.730±0.06 0.790±0.09 Boston 26.38±3.0% 26.62±2.7% 0.295±0.03 0.297±0.03 Diabetes 44.90±12.5% 43.84±10.0% 0.546±0.15 0.592±0.09 Table 2. Zero-one error of NNRank, SVM, GP-MAP, and GP-EP on the eight datasets. SVM denotes the support vector machine method (Shashua & Levin, 2003; Chu & Ghahramani, 2005a). GP-MAP and GP-EP are two Gaussian process methods using Laplace approximation (MacKay, 1992) and expectation propagation (Minka, 2001) respectively (Chu & Ghahramani, 2005a). The results are the average error over 20 trials along with the standard deviation. We use boldface to denote the best results. Data NNRank SVM GP-MAP GP-EP Triazines 52.55±5.0% 54.19±1.5% 52.91±2.2% 52.62±2.7% Pyrimidines 37.71±8.1% 41.46±8.5% 39.79±7.2% 36.46±6.5% Diabetes 44.90±12.5% 57.31±12.1% 54.23±13.8% 54.23±13.8% Machine 17.03±4.2% 17.37±3.6% 16.53±3.6% 16.78±3.9% Auto MPG 27.13±2.0% 25.73±2.2% 23.78±1.9% 23.75±1.7% Boston 26.38±3.0% 25.56±2.0% 24.88±2.0% 24.49±1.9% Stocks 12.68±1.8% 10.81±1.7% 11.99±2.3% 12.00±2.1% Abalone 21.39±0.3% 21.58±0.3% 21.50±0.2% 21.56±0.4% Table 3. Mean absolute error of NNRank, SVM, GP-MAP, and GP-EP on the eight datasets. SVM denotes the support vector machine method (Shashua & Levin, 2003; Chu & Ghahramani, 2005a). GP-MAP and GP-EP are two Gaussian process methods using Laplace approximation and expectation propagation respectively (Chu & Ghahramani, 2005a). The results are the average error over 20 trials along with the standard deviation. We use boldface to denote the best results. Data NNRank SVM GP-MAP GP-EP Triazines 0.730±0.07 0.698±0.03 0.687±0.02 0.688±0.03 Pyrimidines 0.450±0.10 0.450±0.11 0.427±0.09 0.392±0.07 Diabetes 0.546±0.15 0.746±0.14 0.662±0.14 0.665±0.14 Machine 0.186±0.04 0.192±0.04 0.185±0.04 0.186±0.04 Auto MPG 0.281±0.02 0.260±0.02 0.241±0.02 0.241±0.02 Boston 0.295±0.04 0.267±0.02 0.260±0.02 0.259±0.02 Stocks 0.127±0.02 0.108±0.02 0.120±0.02 0.120±0.02 Abalone 0.226±0.01 0.229±0.01 0.232±0.01 0.234±0.01 '}\n"
     ]
    }
   ],
   "source": [
    "print(section_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assume all section names have numbering\n",
    "def construct_markup_with_numbering(sections):\n",
    "    prev_type = ''\n",
    "    ret = ''\n",
    "\n",
    "    numbering_to_prefix = {}\n",
    "\n",
    "    for _, title in sections:\n",
    "        type, value = detect_numbering(title)\n",
    "        if prev_type == '':\n",
    "            numbering_to_prefix[type] = '#'\n",
    "        elif type not in numbering_to_prefix:\n",
    "            numbering_to_prefix[type] = '#' + numbering_to_prefix[prev_type]\n",
    "        \n",
    "        #construct summary\n",
    "        summary = summarize_full_text(section_texts[title])\n",
    "\n",
    "        ret += numbering_to_prefix[type] + ' ' + title + summary + '\\n'\n",
    "        prev_type = type\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "try to generate with model\n",
      "# 1. IntroductionOrdinal regression problem is one of the most challenging problems in machine learning.\n",
      "# 2. MethodA selection of photos from around the world this week:\n",
      "## 2.1. FormulationWe have developed a new method for ordinal re- gression, which can be used to train classification systems.\n",
      "## 2.2. LearningThe cost function for a data point x can be relative entropy or square error between the tar- get vector and the output vector.\n",
      "## 2.3. PredictionThe category of a data point is an important parameter of its classification.\n",
      "# 3. Experiments and ResultsA selection of photos from around the world this week:\n",
      "## 3.1. Benchmark Data and Evaluation MetricA neural network approach to ordinal regression is presented.\n",
      "## 3.2. Comparison with Neural NetworkA new ordinal regression neural network method is presented in this paper.\n",
      "## 3.3. Comparison with Gaussian Processes andWe present a new ordinal regression method that achieves the best results on eight datasets.\n",
      "# 4. Discussion and Future WorkIn our work, we have proposed two directions for the design of neural networks for ordinal regression.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "markup = construct_markup_with_numbering(sections)\n",
    "print(markup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_markmap(markup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "MARKMAP_PATH = \"/Users/evelinewang/.nvm/versions/node/v22.14.0/bin/markmap\"\n",
    "\n",
    "def generate_markmap(markdown_string, output_file=\"markmap.html\"):\n",
    "    with open(\"temp.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(markdown_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(node:68875) ExperimentalWarning: The ESM module loader is experimental.\n",
      "internal/modules/run_main.js:54\n",
      "    internalBinding('errors').triggerUncaughtException(\n",
      "                              ^\n",
      "\n",
      "Error [ERR_MODULE_NOT_FOUND]: Cannot find package 'fs' imported from /Users/evelinewang/.nvm/versions/node/v22.14.0/lib/node_modules/markmap-cli/dist/cli.js\n",
      "    at packageResolve (internal/modules/esm/resolve.js:594:9)\n",
      "    at moduleResolve (internal/modules/esm/resolve.js:633:14)\n",
      "    at Loader.defaultResolve [as _resolve] (internal/modules/esm/resolve.js:726:11)\n",
      "    at Loader.resolve (internal/modules/esm/loader.js:97:40)\n",
      "    at Loader.getModuleJob (internal/modules/esm/loader.js:243:28)\n",
      "    at ModuleWrap.<anonymous> (internal/modules/esm/module_job.js:47:40)\n",
      "    at link (internal/modules/esm/module_job.js:46:36) {\n",
      "  code: 'ERR_MODULE_NOT_FOUND'\n",
      "}\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/Users/evelinewang/.nvm/versions/node/v22.14.0/bin/markmap', 'temp.md', '-o', 'markmap.html']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_markmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkdown_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mgenerate_markmap\u001b[0;34m(markdown_string, output_file)\u001b[0m\n\u001b[1;32m      8\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(markdown_string)  \u001b[38;5;66;03m# Save Markdown file\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Convert Markdown to Markmap HTML\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMARKMAP_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemp.md\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-o\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Markmap saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/subprocess.py:571\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 571\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    572\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/Users/evelinewang/.nvm/versions/node/v22.14.0/bin/markmap', 'temp.md', '-o', 'markmap.html']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "generate_markmap(markdown_string)\n",
    "#I didn't manage to make this work so I ran 'markmap temp.md -o markmap.html' on my terminal to generate directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: nltk in /Users/ruoqwang/Library/Python/3.12/lib/python/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /Users/ruoqwang/Library/Python/3.12/lib/python/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/ruoqwang/Library/Python/3.12/lib/python/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/ruoqwang/Library/Python/3.12/lib/python/site-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/ruoqwang/Library/Python/3.12/lib/python/site-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ruoqwang/Library/Python/3.12/lib/python/site-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/ruoqwang/Library/Python/3.12/lib/python/site-packages (from nltk->rouge_score) (4.67.1)\n",
      "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24986 sha256=5602dff33df9802b737573ed816dddc27217ba27079b698afa92a890dae606db\n",
      "  Stored in directory: /Users/ruoqwang/Library/Caches/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: absl-py, rouge_score\n",
      "Successfully installed absl-py-2.2.2 rouge_score-0.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "from evaluate import load\n",
    "import bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: 0.39\n",
      "ROUGE-2: 0.103\n",
      "ROUGE-L: 0.211\n",
      "BLEU: 0.038\n",
      "BERTScore-F1: 0.833\n"
     ]
    }
   ],
   "source": [
    "#copied from Eka's notebook\n",
    "# Initialize metrics\n",
    "rouge = load(\"rouge\")\n",
    "bleu = load(\"bleu\")\n",
    "# Sample input\n",
    "abstract = \"\"\"Ordinal regression is an important type of learning, which has properties of\n",
    "both classification and regression. Here we describe a simple and effective\n",
    "approach to adapt a traditional neural network to learn ordinal categories. Our\n",
    "approach is a generalization of the perceptron method for ordinal regression.\n",
    "On several benchmark datasets, our method (NNRank) outperforms a neural network\n",
    "classification method. Compared with the ordinal regression methods using\n",
    "Gaussian processes and support vector machines, NNRank achieves comparable\n",
    "performance. Moreover, NNRank has the advantages of traditional neural\n",
    "networks: learning in both online and batch modes, handling very large training\n",
    "datasets, and making rapid predictions. These features make NNRank a useful and\n",
    "complementary tool for large-scale data processing tasks such as information\n",
    "retrieval, web page ranking, collaborative filtering, and protein ranking in\n",
    "Bioinformatics.\"\"\"\n",
    "\n",
    "generated_summary = \"\"\"1. Introduction: Ordinal regression problem is one of the most challenging problems in machine learning.\n",
    "2. Method: A selection of photos from around the world this week:\n",
    "2.1. Formulation: We have developed a new method for ordinal re- gression, which can be used to train classification systems.\n",
    "2.2. Learning: The cost function for a data point x can be relative entropy or square error between the tar- get vector and the output vector.\n",
    "2.3. Prediction: The category of a data point is an important parameter of its classification.\n",
    "3. Experiments and Results: A selection of photos from around the world this week:\n",
    "3.1. Benchmark Data and Evaluation Metric: A neural network approach to ordinal regression is presented.\n",
    "3.2. Comparison with Neural Network: A new ordinal regression neural network method is presented in this paper.\n",
    "3.3. Comparison with Gaussian Processes and: We present a new ordinal regression method that achieves the best results on eight datasets.\n",
    "4. Discussion and Future Work: In our work, we have proposed two directions for the design of neural networks for ordinal regression.\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_automatic_metrics(abstract, generated_summary):\n",
    "    results = {}\n",
    "\n",
    "    # ROUGE\n",
    "    rouge_scores = rouge.compute(predictions=[generated_summary], references=[abstract])\n",
    "    results[\"ROUGE-1\"] = round(rouge_scores[\"rouge1\"], 3)\n",
    "    results[\"ROUGE-2\"] = round(rouge_scores[\"rouge2\"], 3)\n",
    "    results[\"ROUGE-L\"] = round(rouge_scores[\"rougeL\"], 3)\n",
    "\n",
    "    # BLEU\n",
    "    bleu_score = bleu.compute(predictions=[generated_summary], references=[[abstract]])\n",
    "    results[\"BLEU\"] = round(bleu_score[\"bleu\"], 3)\n",
    "\n",
    "    # BERTScore\n",
    "    P, R, F1 = bert_score.score([generated_summary], [abstract], lang=\"en\", verbose=False)\n",
    "    results[\"BERTScore-F1\"] = round(F1[0].item(), 3)\n",
    "\n",
    "    return results\n",
    "\n",
    "auto_scores = evaluate_automatic_metrics(abstract, generated_summary)\n",
    "for metric, score in auto_scores.items():\n",
    "    print(f\"{metric}: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
